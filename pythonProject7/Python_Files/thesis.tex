\documentclass[%
thesis=student,% bachlor's or master's thesis
coverpage=false,% do not print an extra cover page
titlepage=false,% do not print an extra title page
headmarks=true, % headmarks can be switched on or off
english,% or `german`
font=libertine, % use `libertine` font; alternatives: `helvet` / `palatino` / `times`
math=newpxtx, % math font `newpxtx`; alternatives: `ams`, `pxtx`
BCOR=5mm,% binding correction - adapt accordingly
coverBCOR=11mm% binding correction for the cover - adapt accordingly
]{tum-templates/book/tumbook}

\makeatletter %redefine some labels from the TUM template
\provideName{\@tum@examiner@}{Supervisor}{Themensteller} % or `Themenstellerin`
\provideName{\@tum@supervisor@}{Advisors}{Betreuer} % or `Advisor` / `Betreuerin`
\makeatother

\usepackage{booktabs}% for more beautiful tables
\usepackage{cleveref}% intelligent references

\usepackage{diffcoeff}
\usepackage{amsmath}
\usepackage{graphicx}

\usepackage{xcolor}
\usepackage{colortbl}

\usepackage{caption}
\usepackage{float}

\usepackage{listings}

\usepackage{algorithm}
\usepackage{algpseudocode}

\lstdefinestyle{pythonstyle}{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black}\itshape,
    stringstyle=\color{red},
    showstringspaces=false,
    numbers=left,
    numberstyle=\tiny\color{gray},
    numbersep=8pt,
    frame=single,  
    framerule=0.5pt,
    framexleftmargin=15pt,
    xleftmargin=25pt,
    xrightmargin=5pt,
    breaklines=true,
    breakatwhitespace=true,
    breakindent=20pt,
    tabsize=4,
    captionpos=t, 
    belowcaptionskip=\baselineskip,
    abovecaptionskip=0pt,
    columns=flexible,
    keepspaces=true,
    escapeinside={(*}{*)},
}

\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}[definition]{Satz}

%Literatur
\usepackage[%
    backend=bibtex, %, or `biber` on more up-to-date systems
    sortcites, % sort automatically
    sorting=nty, % sort order
    safeinputenc, % solves problems with unicode-formatted author names etc.
    citestyle=alphabetic, %
    bibstyle=alphabetic, %
    hyperref=true, % provide clickable links
    maxbibnames=3, % shorten author list for more than 3 names
    maxcitenames=3, % use at most 3 names for key
    url=false, % do not print URLs
    doi=false, % do not print DOIs
    giveninits=true,
    ]%
{biblatex}
\addbibresource{literature.bib}

% automatische Anführungszeichen
\usepackage[autostyle=true]{csquotes}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue!50!black,
    citecolor=green!50!black,
    urlcolor=red!50!black,
    bookmarksnumbered=true,
    pdftitle={Stability of LLMs for Code Under Semantic-Preserving Perturbations},
    pdfauthor={Florian Hirt},
    pdfsubject={Master Thesis},
    pdfkeywords={LLM, Code Generation, Adversarial Robustness}
}


\title{Stability of LLMs for Code Under Semantic-Preserving Perturbations}
\subtitle{With potential Adversarial Attacks}

\author{Florian Hirt}

\degree{Master of Science}% or `Bachlor of Science`
\dateSubmitted{16th June 2025}% preferably use some universally recognized date format


\examiner{Günnemann, Stephan}% `Themensteller`
\supervisor{Geisler, Simon; Schwinn, Leo}% `Betreuer`


\begin{document}

\frontmatter
\maketitle

\section*{Zusammenfassung}
Große Sprachmodelle (Large Language Models, LLMs) haben bemerkenswerte Fähigkeiten bei der Codegenerierung und beim Verstehen von Aufgaben bewiesen, doch ihre Robustheit gegenüber semantisch äquivalenten Codevariationen ist noch wenig erforscht. Diese Arbeit untersucht die Stabilität modernster LLMs bei der Verarbeitung von Code, der so abgeändert wird dass sich der Inputtext verändert, die Semantik jedoch erhalten bleibt. Insbesondere fokussieren wir uns auf die Umordnung von Anweisungen und die Vertauschung kommutativer Operationen. Wir entwickeln ein umfassendes Framework, das erweiterte Programmgraphen und topologische Sortierung nutzt, um systematisch gültige Codetransformationen zu erzeugen, die die Programmsemantik bewahren und gleichzeitig die Oberflächensyntax verändern. Durch Experimente mit mehreren Datensätzen, darunter unser angepasster CodeScope-Benchmark und der SAFIM-Fill-in-the-Middle-Datensatz, zeigen wir, dass aktuelle LLMs teils eine erhebliche Anfälligkeit für diese Transformationen aufweisen. Bei der LLM-as-a-Judge-Evaluierung finden wir Leistungseinbußen von 27-52 \% bei verschiedenen Modellen, während die Unit-Test-basierte Evaluierung 5-10 \% Anfälligkeit zeigt - was darauf hindeutet, dass die Modelle eher mit stilistischer Konsistenz als mit funktionaler Korrektheit zu kämpfen haben. Überraschenderweise korreliert die Modellgröße nicht mit der Robustheit: größere Modelle wie Gemma 3 12B zeigen eine höhere Anfälligkeit (52,7 \%) als kleinere Modelle. Unsere Ergebnisse zeigen, dass einige der aktuellen LLMs trotz beeindruckender Benchmark-Leistung eher auf oberflächlichen Mustern als auf echtem semantischem Verständnis beruhen, was erhebliche Auswirkungen auf ihren Einsatz in Produktionsumgebungen hat, in denen Code-Korrektheit und Sicherheit von größter Bedeutung sind.

\section*{Abstract}
Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation and understanding tasks, yet their robustness to semantically equivalent code variations remains poorly understood. This thesis investigates the stability of state-of-the-art LLMs when processing code that has been transformed through semantic-preserving perturbations, specifically statement reordering and commutative operation swapping. We develop a comprehensive framework leveraging enhanced program graphs and topological sorting to systematically generate valid code transformations that preserve program semantics while altering surface syntax. Through experiments on multiple datasets, including our adapted CodeScope benchmark and the SAFIM fill-in-the-middle dataset, we reveal that some of the current LLMs exhibit significant vulnerability to these transformations. Using LLM-as-a-judge evaluation, we find performance degradations of 27-52 \% across different models, while unit test-based evaluation shows 5-10 \% vulnerability rates—suggesting models struggle more with stylistic consistency than functional correctness. Surprisingly, model size does not correlate with robustness: larger models like Gemma 3 12B show higher vulnerability (52.7 \%) than smaller counterparts. Our findings indicate that despite impressive benchmark performance, some of the current LLMs rely on surface-level patterns rather than true semantic understanding, with significant implications for their deployment in production environments where code correctness and security are paramount.
\cleardoublepage{}

\tableofcontents

\mainmatter{}

\chapter{Introduction}

Large Language Models have revolutionized software development by demonstrating remarkable capabilities in code generation, completion, and translation tasks. Models like ChatGPT by OpenAI and open-source alternatives like Llama by Meta have achieved significant advances across various coding tasks, with several benchmarks tracking their improving performance \cite{Chen2021} \cite{Yan2023}. This rapid advancement has led to unprecedented industry adoption—software development has the highest rate of LLM adoption among industries, with nearly 25\% of companies implementing LLM-based GenAI initiatives \cite{France2024}. According to a 2023 Stack Overflow survey of over 90,000 developers, more than 70\% were using or planned to use LLM-based tools for coding.

However, as these models become increasingly integrated into production environments and development workflows, fundamental questions arise about their reliability and robustness. While benchmark scores continue to improve, it remains unclear whether these models truly understand code semantics or merely exploit surface-level patterns from their training data. This distinction becomes crucial when considering deployment in production environments where code correctness and security are fundamental.

Current research reveals concerning limitations: LLMs are susceptible to adversarial inputs that can induce the generation of functionally correct but insecure code, with studies showing that "LLMs provide nondeterministic responses, incorrect and unfaithful reasoning, and perform poorly in real-world scenarios" \cite{Ullah2023}. Notably, fewer than 50\% of developers report that they "somewhat trust" or "highly trust" these AI coding tools \cite{France2024}, indicating a significant gap between adoption rates and confidence in reliability.

This thesis addresses a specific aspect of LLM robustness that has significant implications for code understanding: stability under semantic-preserving perturbations. We investigate whether current state-of-the-art LLMs exhibit similar stability to meaningless statement reorderings that preserve program semantics, building on Geisler's work on graph construction for source code \cite{Geisler2023}. Recent work by \textcite{Lam2024} has demonstrated "critical robustness issues of LLMs in code execution and understanding" under structural and semantic perturbations. Our work extends this line of inquiry by focusing specifically on dependency-preserving transformations that should, in principle, be recognized as equivalent by any system with true semantic understanding.

Through systematic analysis of statement reordering and commutative operation swapping, we reveal critical insights into whether current state-of-the-art models genuinely comprehend code structure or merely exploit memorized patterns from their training data. 


\section{Related work}
This section provides a review of the relevant literature, establishing a foundation for the research presented in this thesis.

\subsection{Large Language Models for Code Generation}

A recent and comprehensive overview on Code Generation with LLMs is provided by \textcite{Jiang2024} where the author papers on code generation and gives an overview on current benchmarks, model architectures and their applications. Their historical analysis tracks the evolution of LLM performance in code tasks, with HumanEval benchmark scores improving from just 3.6\% with early models to more than 95\% with recent architectures. 

Several studies have examined the factors contributing to this dramatic improvement. \textcite{Chen2021} introduced Codex and the HumanEval benchmark, establishing key evaluation metrics for code generation. \textcite{Austin2021} complemented this with the Mostly Basic Programming Problems (MBPP) dataset, providing a broader range of programming challenges. 

The survey distinguishes between general-purpose LLMs like ChatGPT and GPT-4 versus code-specialized models such as StarCoder \cite{Li2023}, Code LLaMA \cite{Roziere2023}, DeepSeek-Coder \cite{Guo2024} highlighting how the latter group optimizes for code-centric tasks through specialized pre-training and fine-tuning approaches. Recent work by \textcite{Lozhkov2024} on StarCoder2 further demonstrates the benefits of training on curated code datasets with careful data deduplication and quality filtering.

This directly relates to our investigation of model stability under semantic-preserving transformations, as it raises questions if models, which overall perform better on certain benchmark scores, for example with respect to a pass@k rate, are also more robust.


\subsection{Adversarial Examples in Code Domains}
Adversarial attacks on code present distinct challenges compared to natural language. Code must maintain syntactic validity and semantic correctness while achieving the attack objective. \textcite{Zhang2019} provide a systematical study of adversarial examples for code, focusing on variable renaming and dead code insertion.
\subsubsection{Adversarial Transformations and Attack Methods}
Several papers have explored transformations that preserve code semantics. For example \textcite{Yefet2020} demonstrated that models trained on code are vulnerable to adversarial examples generated through variable and function renaming.
More recently, \textcite{Sarker2024} showed that code LLMs are highly sensitive to minor perturbations in mathematical formulas while preserving semantic meaning. \textcite{Rabbi2024} found that docstring modifications, function name changes, and formatting alterations significantly impact model performance across different programming languages \cite{Rabbi2024}.

Recent advances in automated adversarial attack generation have significantly expanded the threat landscape. GraphCodeAttack \cite{Nguyen2024} represents a breakthrough by automatically mining discriminative AST patterns for adversarial example generation, achieving 30\% improvement over CARROT and 33\% over ALERT. This framework leverages separate pre-trained code models to synthesize concrete attacks from abstract patterns. Similarly, DeceptPrompt \cite{Wu2023} introduces adversarial natural language instructions using evolution-based algorithms, achieving a 50\% improvement in attack success rates when targeting models like CodeLlama and StarCoder. These automated approaches move beyond hand-crafted transformations to systematically discover effective attack vectors.

The evaluation of adversarial robustness has also matured significantly. ReCode \cite{Wang2022} provides a comprehensive benchmark with over 30 semantic-preserving transformations, revealing that models are most sensitive to syntax perturbations with large performance degradation despite maintaining identical functionality. LiveCodeBench \cite{Jain2024} addresses the critical issue of data contamination in evaluation by implementing temporal segmentation, showing that many models' apparent improvements may result from overfitting to leaked training data rather than genuine capability advances.


\subsubsection{Code Obfuscation as Adversarial Examples}
The relationship between code obfuscation and adversarial examples has been explored by several researchers. Bielik and Vechev studied how code obfuscation techniques affect neural models for code completion \cite{Bielik2020}. Their findings suggest that models rely heavily on surface-level patterns rather than deeper semantic understanding.


\subsection{Robustness of Machine Learning Models for Code}

Despite their impressive capabilities, LLMs for code exhibit significant robustness issues when faced with semantically equivalent inputs. \textcite{Zhong2024} demonstrated that even advanced models like GPT-4 generate code with API misuse in 62\% of cases when handling real-world coding questions, which would pose critical issues to real-life applications. \\
Similar concerns emerge in studies of syntactic robustness. \textcite{Sarker2024} showed that code LLMs are highly sensitive to minor perturbations in mathematical formulas while preserving semantic meaning. 
This sensitivity extends beyond syntax to natural language variations in prompts, with recent work by researchers documenting performance drops of up to 21.2\% when prompts contain realistic linguistic variations \cite{Chen2024}. 
The vulnerability pattern is consistent across different programming languages, with studies revealing that docstring modifications, function name changes, and formatting alterations all significantly impact model performance \cite{Rabbi2024}. 
These findings align with Ullah's observation that LLMs provide "nondeterministic responses, incorrect and unfaithful reasoning, and perform poorly in real-world scenarios" \cite{Ullah2023}, emphasizing the need for more robust approaches to code generation. \\
This also relates to our own work. While these studies provide valuable insights into various dimensions of LLM robustness for code, our work specifically addresses the critical research gap regarding LLM stability towards semantically equivalent code transformations such as statement reordering and commutative operation swapping, building on \textcite{Geisler2023}'s graph-based approach to inform potential adversarial examples and robustness improvements.

\subsubsection{Evaluation Frameworks and Benchmarks}

The development of comprehensive evaluation frameworks has revealed systematic vulnerabilities previously hidden by traditional metrics. L2CEval \cite{Ni2023} provides a systematic comparison of 54 models across different organizations, revealing that model size alone does not guarantee robustness and that instruction tuning can sometimes reduce adversarial robustness despite improving standard performance. CodeJudge \cite{Tong2024} introduces an LLM-as-judge approach that eliminates the need for extensive test case creation while maintaining high correlation with human evaluation.

Security-focused evaluation frameworks have uncovered additional dimensions of vulnerability. SecurityEval \cite{siddiq2022seceval} demonstrates that traditional functionality metrics miss critical vulnerabilities in generated code, with models frequently generating syntactically correct but security-vulnerable code. BigCodeBench \cite{Zhuo2025} emphasizes complex, practical programming tasks with diverse function calls, showing significant performance degradation on realistic tasks compared to synthetic benchmarks like HumanEval.

\subsection{Semantic-Preserving Code Transformations}

The concept of semantic-preserving transformations has deep roots in compiler optimization theory and software engineering, but its application to testing LLM robustness reveals fundamental limitations in model understanding.

\subsubsection{Compiler Optimizations}
The concept of semantic-preserving transformations has deep roots in compiler optimization theory. Many papers on Compiler Optimization describe numerous transformations that preserve program semantics while improving performance. These include constant folding, dead code elimination, and loop unrolling. A notable contribution in this area is is \cite{Buchwald2011}, which also uses a graph representation of code for their compiler optimization tool.
\subsubsection{Software Refactoring Techniques}
Software engineering research has extensively studied refactoring operations that improve code quality while preserving functionality \cite{Fowler2002}. Automated refactoring tools implement various semantic-preserving transformations, from simple variable renaming to complex structural reorganizations.


\subsection{Graph Representations for Code Analysis}
Because of the aforementioned limitations, researchers have looked into modeling code with a graph-based approach. 

\textcite{Borowski2023} propose the Semantic Code Graph (SCG), an information model specifically designed to accelerate code comprehension by providing "a detailed abstract representation of code dependencies with a close relationship to the source code". \\

\textcite{Bieber2022} introduce a versatile library that constructs graph representations of Python programs using static analysis, supporting control-flow graphs, data-flow graphs, and composite "program graphs" that combine multiple types of information \cite{Bieber2022}. These graph-based representations have been used for machine learning tasks on code, for example also for transformers on directed graphs \cite{Geisler2023}. \\
This library serves the present implementation, as is shown later on.

\subsubsection{Advanced Graph-Based Approaches}

Recent advances in graph-based code representations have demonstrated significant improvements in robustness and understanding. AST-T5 \cite{Gong2024_2} integrates Abstract Syntax Tree information directly into the T5 architecture, achieving 2-point improvements in exact match scores for code repair tasks and 3-point improvements for cross-language transpilation. This structure-aware pretraining demonstrates that incorporating syntactic information can enhance both performance and robustness.

The development of hybrid approaches combining language models with graph neural networks has proven particularly effective. Vul-LMGNNs \cite{Liu2025} fuses language models with graph neural networks through online knowledge distillation, outperforming 17 state-of-the-art approaches on real-world vulnerability datasets. This suggests that neither purely sequential nor purely structural approaches alone are sufficient for robust code understanding.

\section{Research Questions}
Based on the motivation outlined above, this thesis addresses the following research questions:

\begin{enumerate}
    \item \textbf{RQ1}: To what extent are current state-of-the-art LLMs robust to semantic-preserving transformations in code?
    \item \textbf{RQ2}: Can we systematically generate adversarial examples that reveal weaknesses in LLM code understanding through statement reordering and commutative operation swapping?
    \item \textbf{RQ3}: Do models that perform better overall on benchmarks tasks also demonstrate greater robustness to semantic-preserving perturbations?
\end{enumerate}

\chapter{Background}

This chapter introduces the theoretical concepts, that we will use later on. Besides the basic concepts in machine learning they also include more advanced topics like Large Language Models (LLM), Adversarial Attacks, Abstract Syntax Trees (AST) and topological sorts on a graph.

\section{LLMs}

Large Language Models (LLMs) have achieved great success on a variety of natural language tasks within the last years and have gotten more attention since the release of Chat-GPT in November 2022. \cite{OpenAI2022} \\
Their ability of general-purpose language understanding and generation comes by training billions of parameters on large amounts of data. LLMs became the current state of the art in Natural Language Processing (NLP), through the attention mechanism and transformer architecture, introduced in 2017, which then enabled a scaling from small to large language models. 

\subsection{Foundations and Evolution}

Modeling, Understanding and Generating Natural Language has been an objective for Machine Learning for the past decades. 
The evolution of language models has come from early statistical approaches to today's massive neural architectures. \textcite{Minaee2025} note, this journey can be understood as four waves of development: statistical language models, neural language models, pre-trained language models, and finally, the current era of large language models. 

\subsubsection{SLMs}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{Words_as_tokens.png}
    \caption{Tokenization process for natural language text. The sentence "John is eating lunch." is broken down into individual tokens that serve as input units for language models. Each token represents a word or punctuation mark that the model processes independently before considering relationships between them.}
    \label{fig:Words_as_tokens}
    \cite{Günnemann2023}
\end{figure}

Statistical language models (SLMs) were introduced in the 1950s, beginning with Shannon's pioneering work applying information theory to natural language \cite{Shannon1951}. These early models viewed text as a sequence of tokens (shown in \Cref{fig:Words_as_tokens}) and calculated probabilities based on n-gram patterns, which consider only a fixed window of previous words when predicting the next token. \\

For example a Unigram ($n = 1$) model would only consider the single tokens in \Cref{fig:Words_as_tokens} as "John", "is", "eating", "lunch", "." and create the isolated probabilities for $\mathbb{P}(X_i)$, where $X_i$ are the individual tokens. These probabilities are estimated from their frequencies in the training corpus, with $\mathbb{P}(X_i) = \frac{\text{count}(X_i)}{\text{total tokens}}$. This approach entirely ignores word order and context, making it extremely limited for natural language modeling. \\

A more powerful approach is the Bigram ($n = 2$) model, which considers pairs of consecutive words, modeling the conditional probability $\mathbb{P}(X_i | X_{i-1})$ of a word given its predecessor. For our example, a bigram model would calculate probabilities like $\mathbb{P}(\text{"is"} | \text{"John"})$ and $\mathbb{P}(\text{"eating"} | \text{"is"})$. This captures local dependencies and significantly improves the model's ability to generate coherent text. The probability of the entire sequence "John is eating lunch." would be calculated as:

\begin{equation}
\mathbb{P}(\text{sequence}) = \mathbb{P}(\text{"John"}) \cdot \mathbb{P}(\text{"is"} | \text{"John"}) \cdot \mathbb{P}(\text{"eating"} | \text{"is"}) \cdot \mathbb{P}(\text{"lunch"} | \text{"eating"}) \cdot \mathbb{P}(\text{"."} | \text{"lunch"})
\end{equation}

Hidden Markov Models (HMMs) extend this idea by introducing additionally to the observed tokens $X_1, X_2,...$ hidden states $Z_1, Z_2, ...$ that generate those tokens. Each state has emission probabilities (generating visible tokens) and transition probabilities (moving between states). In natural language processing, these hidden states might correspond to abstract semantic concepts. 
The hidden states satisfy the Markov property (transition probabilities). The distribution of $X_t$ depends only on $Z_t$ (emission probabilities).
\begin{align}
\mathbb{P}(Z_{t+1}|Z_t,Z_{t-1},...Z_1) &= \mathbb{P}(Z_{t+1}|Z_t) \\
\mathbb{P}(X_{t+1}|Z_1,...,Z_T,X_1,...,Z_T) &= \mathbb{P}(X_{t+1}|Z_{t+1})
\end{align}

HMMs can model longer-range dependencies than simple n-gram models while maintaining computational tractability through the Markov assumption that the current state depends only on the previous state. These simple models were already applied to tasks like autocompletion, where the most likely word for the current sequence of words would be suggested. \\
While being fundamental to many NLP applications today, these models struggled with long-range dependencies and suffered from data sparsity issues.

\subsubsection{NLMs}
Neural language models (NLMs) marked a significant advancement in the 2010s. Word embeddings like Word2Vec \cite{Mikolov2013} represented words as dense vectors (in contrast to sparse one-hot encodings) in a continuous space, capturing semantic relationships. \\

These were followed by recurrent neural networks (RNNs) and their improvements such as long-short-term memory (LSTM) or gated recurrent unit (GRU) networks, which could theoretically capture dependencies of arbitrary length. \\

RNNs fundamentally differ from n-gram models by maintaining a hidden state that acts as a memory of past inputs, regardless of how far back they occurred. At each time step $t$, an RNN processes an input token $x_t$ (represented as a word embedding) together with the previous hidden state $h_{t-1}$ to produce:
\begin{equation}
    h_t = f(z_t)
\end{equation}
with
\begin{equation}
    z_t = Wh_{t-1} + Ux_t + b
\end{equation}
and the output token
\begin{equation}
    \hat{y}_t = \text{softmax}(o_t)
\end{equation}
with
\begin{equation}
    o_t = Vh_t
\end{equation}

Where $f$ is a non-linear activation function (typically tanh or ReLU), $W$, $U$, and $V$ are learned weight matrices, and $b$ is a bias term. All these parameters are shared over time. The output $\hat{y}_t$ represents the probability distribution over the next token $\mathbb{P}(y_t|x_1,...,x_t)$

NLMs adressed both the main issues with SLMs: Unlike SLMs where the context window size is fixed by the value of $n$, RNNs can theoretically consider the entire sequence history through the continuously updated hidden state. This addresses the fundamental limitation of n-gram models by allowing the network to capture long-range dependencies.

Additionally, the distributed representation of words and states enables better generalization. While n-gram models treat words as discrete units with no inherent relationship, word embeddings in neural models place semantically similar words near each other in the vector space, allowing the model to make reasonable predictions even for rare or unseen word combinations.

However, these architectures still face limitations - which mainly consider speed. Since these models are recurrent in the way that each hidden state depends on the last hidden state, they are not parallelizable, which makes them hard to scale with more parameters and training data.

\subsubsection{PLMs}

The third wave brought pre-trained language models. These approaches used the transformer architecture introduced by \textcite{Vaswani2017}, which enabled more efficient training through parallel processing. This solved the problem that previous NLMs had and made it possible to scale these models.\\
What characterized these new PLMs was the pre-training and fine-tuning concept, where models are first trained on large data with self-supervised objectives and then fine-tuned for specific downstream tasks, like for example coding.

\subsubsection{LLMs}


The current wave of large language models (LLMs) began when researchers discovered that scaling up model size, data, and compute according to power laws led to emergent capabilities not seen in smaller models. Models like GPT-3 demonstrated that with sufficient scale, language models could perform complex tasks. \cite{OpenAI2022} \\
This scaling phenomenon has continued with models like GPT-4, Claude, and LLaMA, which have shown increasingly effective reasoning abilities across diverse domains, including programming. \\


\subsection{The Transformer Architecture}

The aforementioned Transformer architecture by \textcite{Vaswani2017}, revolutionized sequence modeling for PLMs and is still the foundation for today's LLMs. To make parallelization possible, it eliminates recurrence entirely in favor of self-attention mechanisms. Unlike RNNs which process tokens sequentially and struggle with long-range dependencies due to vanishing gradients, Transformers compute attention weights between all positions in a sequence, allowing direct modeling of relationships regardless of distance.

The architecture consists of two main components: an encoder stack (left) and a decoder stack (right). 

The Transformer model processes sequences through stacked encoder and decoder layers. Each encoder layer consists of two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The decoder adds a third sub-layer that performs multi-head attention over the encoder output. Crucially, residual connections surround each sub-layer, followed by layer normalization, expressed as $\text{LayerNorm}(x + \text{Sublayer}(x))$. To maintain model efficiency, all sub-layers produce outputs of dimension $d_{model} = 512$.

The key innovation lies in replacing recurrence with self-attention, enabling parallel computation while maintaining the ability to model dependencies between any positions in the sequence. This parallelization is particularly important for our research, as it allows models to process entire code blocks simultaneously, potentially missing dependencies that our perturbations exploit.

Each stack contains multiple identical layers, with each encoder layer comprising two sub-layers:
\begin{enumerate}
    \item A multi-head self-attention mechanism
    \item A position-wise fully connected feed-forward network
\end{enumerate}

The decoder incorporates an additional sub-layer performing multi-head attention over the encoder output. Residual connections surround each sub-layer, followed by layer normalization, expressed as $\text{LayerNorm}(x + \text{Sublayer}(x))$.

\subsubsection{Multi-Head Attention Mechanism}

The core innovation, multi-head self-attention, enables the model to attend to information from different representation subspaces at different positions simultaneously. Rather than computing a single attention function, the model employs $h$ parallel attention heads:

\begin{equation}
    \text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
\end{equation}

where each head is computed as:
\begin{equation}
    \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{equation}

The projection matrices $W_i^Q \in \mathbb{R}^{d_{model} \times d_k}$, $W_i^K \in \mathbb{R}^{d_{model} \times d_k}$, $W_i^V \in \mathbb{R}^{d_{model} \times d_v}$, and $W^O \in \mathbb{R}^{hd_v \times d_{model}}$ are learned parameters. This multi-head approach allows the model to jointly attend to information from different representation subspaces.

\subsubsection{Positional Encoding}

Since the architecture lacks inherent sequential ordering, positional encodings are added to input embeddings to inject information about token positions. The original implementation uses sinusoidal functions:

\begin{equation}
    PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{model}})
\end{equation}
\begin{equation}
    PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d_{model}})
\end{equation}

where $pos$ represents the position and $i$ represents the dimension. This formulation enables the model to learn relative positions and generalize to sequence lengths unseen during training.

\subsubsection{Impact on Code Understanding}

The Transformer architecture's relevance to code understanding tasks stems from several key properties:
\begin{itemize}
    \item \textbf{Global context}: Unlike sequential models, Transformers can directly model long-range dependencies crucial for understanding code semantics across function boundaries
    \item \textbf{Parallelization}: The absence of recurrence enables efficient training on large code corpora
    \item \textbf{Attention interpretability}: The attention weights provide insights into which code elements the model considers relevant for specific predictions
\end{itemize}

These architectural advantages make Transformers particularly well-suited for the code understanding and generation tasks central to this thesis, where models must capture complex dependencies between distant code elements while maintaining computational efficiency. 

\subsection{Code-Specific LLMs}

While general-purpose LLMs have shown impressive capabilities in code-related tasks, several models have been specifically designed and trained for code understanding and generation:

\subsubsection{Architecture Adaptations}
Code-specific models often incorporate architectural modifications to better capture code structure. Code-specific models incorporate several architectural modifications to better capture code structure. Models like DeepSeek-Coder-V2 extend context windows to support up to 128K tokens \cite{Zhu2024}, while StarCoder2 implements 16,384 token contexts with sliding window attention \cite{Lozhkov2024}. Additionally, these models employ specialized tokenization schemes that account for the different token distributions in programming languages compared to natural language.

\subsubsection{Training Objectives}
Beyond standard language modeling, code-specific models employ specialized training objectives:
\begin{itemize}
    \item \textbf{Fill-in-the-Middle (FIM)}: Training with 90\% FIM ratio enables powerful infilling capabilities without harming left-to-right performance \cite{Bavarian2022}
    \item \textbf{Denoising objectives}: CodeT5+ combines span denoising, contrastive learning, and text-code matching for superior performance \cite{Wang2023}
    \item \textbf{Multi-modal training}: Combining code with documentation, comments, and test cases, as demonstrated by CodeT5+ achieving state-of-the-art results \cite{Wang2023}
\end{itemize}

Notable examples include:
\begin{itemize}
    \item \textbf{StarCoder} \cite{Li2023}: 15.5B parameters trained on 1 trillion tokens, achieving 40\% pass@1 on HumanEval
    \item \textbf{Code Llama} \cite{Roziere2023}: 7B to 70B parameter variants with specialized Python fine-tuning, achieving 67\% pass@1
    \item \textbf{DeepSeek-Coder} \cite{Guo2024, Zhu2024}: Mixture-of-Experts architecture with 236B total but only 21B active parameters
    \item \textbf{StarCoder2} \cite{Lozhkov2024}: 3B-15B models where the 15B variant matches CodeLlama-34B performance
\end{itemize}

These models consistently outperform general-purpose LLMs on code-specific benchmarks, with StarCoder2-15B matching twice-larger models through architectural efficiency \cite{Lozhkov2024}.

\subsubsection{Connection to Research Questions}

The evolution and architecture of LLMs, particularly code-specific models, directly relates to our research questions. 
For \textbf{RQ1}, understanding the architectural choices and training objectives of different code LLMs helps explain their varying levels of robustness to semantic-preserving transformations. Models like StarCoder and Code Llama, despite their specialized training, may still rely on syntactic patterns rather than semantic understanding.

Regarding \textbf{RQ3}, the relationship between model size, architecture, and robustness is not straightforward. While larger models generally perform better on benchmarks, their increased parameter count may also increase the attack surface for adversarial examples, making the correlation between benchmark performance and robustness non-obvious.

\section{Adversarial Attacks}

Adversarial attacks represent a significant challenge to machine learning systems including LLMs. 
They try to exploit vulnerabilities by introducing carefully crafted perturbations to input data that preserve semantic meaning while causing model failures. First these attacks initially targeted computer vision models but have since expanded to natural language processing, GNNs and code understanding domains. \\
For a a definition of the terms, we refer to \cite{Wiyatno2019}:

\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{|p{4cm}|p{11cm}|}
    \hline
    \textbf{Common Terms} & \textbf{Definition} \\
    \hline
    Adversarial example & Input to a machine learning model that is intentionally designed to cause a model to make mistake in its predictions despite resembling a valid input to a human. \\
    \hline
    Adversarial perturbation & Difference between a non-adversarial example and its adversarial counterpart. \\
    \hline
    Adversarial attacks & Methods to generate adversarial examples. \\
    \hline
    Adversarial defenses & Methods to defend against adversarial examples. \\
    \hline
    Adversarial robustness & The property of resisting misclassification of adversarial examples. \\
    \hline
    Adversarial detection & Methods to detect adversarial examples. \\
    \hline
    \end{tabular}
    \caption{Common terms in adversarial machine learning}
    \label{tab:adversarial_terms}
    \cite{Wiyatno2019}
\end{table}

A mathematical formulation would be that for an adversarial example $x'$ the difference to a non-adversarial example $x$ is minimal under a certain metric, while fooling the target model $f$. So
\begin{equation}
    d(x',x) < \epsilon  \qquad \text{such that } \qquad \hat{y}(x') \neq \hat{y}(x)
\end{equation}

where $\epsilon$ denotes a very small constant that bounds the distance and $\hat{y}$ denotes the prediction given by the classifier model $f$.

In the specific context of code completion tasks with LLMs, this mathematical formulation takes on a nuanced interpretation. Here, $x$ represents a code snippet presented to an LLM, while $x'$ is its semantically equivalent but syntactically perturbed variant. The distance metric $d(x',x)$ may be defined through various code similarity measures. \\
We define it as "changing the syntax, but not the semantics".
For code completion models, the prediction $\hat{y}(x)$ represents the model's suggested code continuation, and a successful adversarial attack occurs when $\hat{y}(x') \neq \hat{y}(x)$ despite both inputs having identical functional meaning.

Unlike traditional adversarial examples in image classification where imperceptible pixel modifications can cause drastic misclassifications, code-based adversarial examples must maintain valid syntax and preserve program behavior. This introduces unique constraints that make adversarial attack generation both more challenging and potentially more revealing about model weaknesses. For instance, a model that fails to recognize that $a + b$ and $b + a$ are equivalent in most programming contexts exhibits a fundamental gap in its understanding of code semantics versus surface syntax.

In the context of LLMs for code, adversarial attacks typically involve semantic-preserving transformations, such as variable renaming, statement reordering, or equivalent expression substitution, that maintain program functionality while degrading model performance.
These perturbations exploit the gap between syntactic representation and semantic understanding, revealing fundamental limitations in the way models process the source code. Understanding adversarial vulnerabilities is crucial not only for assessing model robustness but also for developing more reliable models that maintain stable performance across equivalent program variants.

\subsubsection{Connection to Research Questions}

Adversarial attacks form the methodological foundation for examining our research questions. For \textbf{RQ1}, the mathematical formulation of adversarial examples provides the framework for quantifying robustness. In the code domain, this translates to measuring how small syntactic changes can cause large behavioral changes in model output.

\textbf{RQ2} is directly addressed through the systematic generation of adversarial examples. The unique constraints in code (maintaining syntactic validity and semantic equivalence) make adversarial attack generation both more challenging and more revealing about model weaknesses than in other domains.

\section{Abstract Syntax Tree}
\label{sec:AST}

Abstract Syntax Trees (ASTs) represent the hierarchical syntactic structure of source code in a graphical format as a tree. The name comes from the concept of abstracting away irrelevant syntax details and revealing the underlying structure of the code. These vary from different programming languages, such as parentheses, semicolons, and whitespaces. 
Each node in an AST corresponds to a construct in the source code, with the tree structure reflecting the nested relationships between these constructs. Their main application is for compilers in their syntax analysis, but they recently they find new applications in code clone detection or code understanding in Machine Learning. \\
ASTs are particularly relevant to our research on semantic-preserving perturbations, since they allow us to systematically identify transformation opportunities while preserving program meaning. In \Cref{ch:Our_approach} we will go more into detail how we enriched the AST with additional edges to capture all dependencies.\\
Similarly, commutative operations can be identified by their characteristic node patterns in the AST, enabling systematic generation of semantically equivalent but syntactically different code variants. These transformations become the foundation for creating controlled adversarial examples to test the robustness of LLM.


\begin{lstlisting}[style=pythonstyle, caption={A Python function \texttt{test\_function} that takes two parameters and demonstrates control flow with conditional statements. This function will be used to illustrate AST representation and dependency analysis in subsequent figures.}, label=lst:test_function]
def test_function(a, b):
    a = a**2
    if a > 16:
        a = a + 1
    else:
        b = b - 1
    return a + b
\end{lstlisting}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{AST.png}
    \caption{The Abstract Syntax Tree that corresponds to the Python function ~\ref{lst:test_function}}
    \label{fig:AST}
\end{figure}

Figure~\ref{lst:test_function} shows a simple Python function together with its Abstract Syntax Tree representation in Figure~\ref{fig:AST}. The AST illustrates how the program is decomposed into a hierarchical structure. At the root, we have a \texttt{Module} node, which contains a \texttt{FunctionDef} node representing our \texttt{test\_function}. The function node has several child nodes representing its components: \texttt{arguments}, assignment operations (\texttt{Assign}), a conditional statement (\texttt{If}), and a \texttt{Return} statement.

The AST captures the program's logical structure rather than its text. For example, the initial assignment \texttt{a = a**2} is represented as an \texttt{Assign} node with a target (\texttt{Name} node for variable \texttt{a}) and a value expression (\texttt{BinOp} node for the power operation). The conditional statement has three main components: a \texttt{test} expression (\texttt{Compare} node for \texttt{a > 16}), a \texttt{body} for the if-branch, and an \texttt{orelse} field for the else-branch. 

While ASTs provide a valuable representation of code structure, they have significant limitations for semantic-preserving transformations. Most importantly, ASTs capture only syntactic relationships, not semantic dependencies. For example, an AST shows that variable `x` appears in an expression, but doesn't represent that this appearance depends on a previous assignment to `x`. Similarly, an AST represents the textual ordering of two assignment statements, but doesn't encode whether this ordering is semantically significant or arbitrary.

\subsubsection{Connection to Research Questions}

Abstract Syntax Trees are fundamental to our approach for addressing some of our research questions. For \textbf{RQ1}, ASTs provide the structural representation necessary to identify which transformations preserve semantic meaning. By analyzing the AST, we can ensure that our perturbations maintain program correctness while testing model robustness.

\textbf{RQ2} relies heavily on AST manipulation for systematic adversarial example generation. The hierarchical structure of ASTs allows us to identify independent subtrees that can be reordered and nodes representing commutative operations that can be swapped. This systematic approach ensures we explore the space of valid transformations comprehensively.

\section{Topological Sorting}
\label{sec:Topological Sorting}

Topological sorting is a fundamental algorithm for organizing vertices in a directed acyclic graph (DAG) in a sequential order such that for every directed edge $(u, v)$, vertex $u$ comes before vertex $v$ in the ordering. 

\begin{definition}
Given a directed acyclic graph $G = (V, E)$, a topological sort is a sequential ordering of all vertices in $V$ such that for every directed edge $(u, v) \in E$, vertex $u$ comes before vertex $v$ in the ordering.
\end{definition}

Multiple valid topological sorts may exist for a single graph, each representing an alternative but valid ordering of operations that preserves the dependency constraints. This property is particularly useful in code analysis, where different valid execution orders of independent statements can be derived from the same dependency graph.

\subsection{Algorithms for Topological Sorting}

Two principal algorithms exist for computing topological sorts:

\begin{enumerate}
    \item \textbf{Kahn's Algorithm}: This iterative approach repeatedly identifies and removes vertices with no incoming edges (sources), adding them to the result sequence. The algorithm proceeds as follows:
    \begin{itemize}
        \item Identify vertices with no incoming edges
        \item Remove one such vertex and add it to the result sequence
        \item Remove all outgoing edges from that vertex
        \item Repeat until no vertices remain
    \end{itemize}
    If the graph contains a cycle, the algorithm will terminate with vertices remaining, indicating no valid topological sort exists.
    
    \item \textbf{Depth-First Search (DFS)}: This recursive approach uses depth-first traversal of the graph, adding vertices to the result in reverse postorder (appending each vertex after all its descendants have been processed). When implemented with cycle detection, DFS can also identify whether a topological sort is possible.
\end{enumerate}

Both algorithms have a time complexity of $O(|V| + |E|)$, where $|V|$ is the number of vertices and $|E|$ is the number of edges in the graph. \\
The implementation employs Kahn's algorithm. This algorithm is easy to implement and has the advantage that it can easily be modified to a "neighborhood topological sort", which we will define later. We implement Kahn's Algorithm as described above non-deterministical, which provides us with different valid topological sorts each time:


\begin{lstlisting}[style=pythonstyle, caption={Non-deterministic topological sorting algorithm implementation using Kahn's algorithm. The algorithm maintains a queue of source nodes (nodes with no incoming edges) and randomly selects which source to process next, enabling generation of different valid orderings of the same directed acyclic graph.}]
def non_deterministic_topological_sort(graph):
    # Check if the graph is a DAG
    if not nx.is_directed_acyclic_graph(graph):
        raise ValueError("Graph contains a cycle!")

    graph_copy = graph.copy()
    topo_order = []

    # Initialize sources: nodes with no incoming edges
    sources = deque([node for node in graph_copy.nodes
        if graph_copy.in_degree(node) == 0])

    while sources:
        # Randomly select a source node
        random_index = random.randint(0, len(sources) - 1)
        node = sources[random_index]
        sources.remove(node)
        topo_order.append(node)

        successors = list(graph_copy.successors(node))

        for successor in successors:
            graph_copy.remove_edge(node, successor)
            if graph_copy.in_degree(successor) == 0:
                sources.append(successor)

        # Remove the processed node from the graph
        graph_copy.remove_node(node)

    return topo_order
\end{lstlisting}



\subsection{Application to Code Analysis}

In the context of code analysis and transformation, topological sorting provides a principled approach to identifying valid reorderings of program statements. By constructing a dependency graph where

\begin{itemize}
    \item Vertices represent program statements (or parts of them as shown in the previous section)
    \item Directed edges $(u, v)$ indicate that statement $u$ must execute before statement $v$ (due to data or control dependencies)
\end{itemize}

Any valid topological sort of this graph represents a legitimate reordering of statements that maintains the program semantics. However it is important to note here that this mapping is only surjective, not bijective. The reordering of some nodes in the AST does not change the corresponding code. For example underlying nodes that are not on seperate branches, like the for \texttt{Load} nodes in \Cref{fig:AST} are not relevant for the resulting code, because their order is determined by the order of the nodes on the upper levels (\texttt{Assign}, \texttt{If} and \texttt{Compare}). \\
This concept is central to our research on semantic-preserving perturbations, as it enables systematic generation of syntactically different but semantically equivalent code variants.

For instance, in a sequence of assignment statements without interdependencies, multiple orderings are semantically equivalent:

\subsubsection{Connection to Research Questions}

Topological sorting is the algorithmic foundation that enables our investigation of all three research questions. For \textbf{RQ1}, the existence of multiple valid topological sorts for a dependency graph directly quantifies the space of semantic-preserving transformations. The number of valid orderings provides a measure of how much syntactic flexibility exists while maintaining semantic correctness.

\textbf{RQ2} is operationalized through our topological sorting algorithms. The non-deterministic selection in Kahn's algorithm allows systematic exploration of the transformation space, while our neighborhood topological sort enables targeted adversarial search. These algorithms transform the abstract concept of "semantic-preserving perturbations" into concrete, executable transformations.

For \textbf{RQ3}, topological sorting provides the mechanism to generate consistent adversarial examples across different models. By using the same dependency graph and sorting algorithms, we ensure that performance differences reflect model robustness. The correlation between the number of valid topological sorts and model vulnerability also provides insights into which code structures expose model weaknesses.


\begin{lstlisting}[style=pythonstyle, caption={Two examples of semantic-preserving reorderings without any dependencies to consider}]
# Original code
x = 5
y = 10
z = 20

# Semantically equivalent after topological sort
y = 10
x = 5
z = 20
\end{lstlisting}

However, when dependencies exist, topological ordering preserves them:

\begin{lstlisting}[style=pythonstyle, caption={Different valid reorderings, preserving and existing dependency between x and y}]
# Original code
x = 5
y = x + 10  # Depends on x
z = 20

# A valid topological sort - keeping the original code
x = 5
y = x + 10  # Dependency preserved
z = 20

# Another valid topological sort
x = 5
z = 20
y = x + 10  # Still valid because x is defined before it is used

# A third valid option
z = 20
x = 5
y = x + 10  # The condition x before y is still preserved
\end{lstlisting}


Using topological sorting techniques on program dependency graphs, we can systematically generate semantically equivalent code transformations to test the robustness of LLMs in code understanding and generation tasks.


\section{Summary}

This chapter provided the theoretical foundations necessary for understanding our approach:
\begin{itemize}
    \item Large Language Models have shown impressive capabilities in code understanding but may rely on superficial patterns
    \item Adversarial attacks can reveal model vulnerabilities while preserving semantic meaning
    \item Abstract Syntax Trees and program graphs enable systematic code transformations
    \item Topological sorting of dependency graphs allows generation of valid code reorderings
\end{itemize}

These concepts form the basis of our semantic-preserving perturbation framework, which we detail in Chapter~\ref{ch:Our_approach}.

Having established the theoretical foundations of LLMs, adversarial attacks, and program analysis techniques, we now turn to the practical implementation of our approach. The next chapter details how we combine these concepts to create a systematic framework for testing LLM robustness through semantic-preserving code perturbations.

\chapter{Our approach}
\label{ch:Our_approach}
\section{Overview}

\paragraph{Overall goal}

Our goal is to systematically test the robustness of Large Language Models on code understanding and generation tasks by exposing them to semantically equivalent code variants. To achieve this, we need a method to automatically generate valid code transformations that preserve program semantics while varying the surface syntax. This requires solving several technical challenges: identifying which code transformations are semantically valid, ensuring all dependencies between statements are properly tracked, and implementing algorithms to generate diverse variants efficiently. \\

We address these challenges through a pipeline built on program graphs—enriched representations of code that capture not just syntactic structure but also control flow and data dependencies. By carefully analyzing and modifying these graphs, we can identify independent statements that can be safely reordered, detect commutative operations that can be swapped, and generate a large space of semantically equivalent programs. These variants then serve as test cases to evaluate whether LLMs truly understand code semantics or merely memorize syntactic patterns.

\paragraph{Technical requirements}

This chapter describes the technical framework and infrastructure necessary to conduct our experiments on LLM robustness. Before we can systematically evaluate how LLMs respond to semantic-preserving code transformations (addressing RQ1-RQ3), we must first establish a reliable method for generating such transformations automatically.

Our experimental approach requires solving several interconnected foundational challenges. The first challenge involves dependency analysis, where we build enhanced program graphs that accurately capture all semantic dependencies in Python code. This forms the foundation for our second challenge: implementing algorithms for valid transformation generation that can reorder code while guaranteeing semantic preservation. Finally, we develop methods for adversarial search to systematically explore the space of transformations and identify those that maximally degrade model performance. These technical components work together to enable reproducible and systematic evaluation of LLM robustness.

The following sections detail our technical infrastructure in three parts:
\begin{enumerate}
    \item \textbf{\Cref{sec:AST_manipulations} and \Cref{sec:Code_reorderings_through_sorts}}: Foundation components adapted and extended from existing work, particularly the \texttt{python\_graphs} framework by \textcite{Bieber2022} and dependency analysis from \textcite{Geisler2023}
    \item \textbf{\Cref{sec:Neighborhood_sampling}}: Our novel contribution of neighborhood sampling for targeted adversarial example generation
\end{enumerate}

While these components are prerequisites rather than direct answers to our research questions, they are essential for enabling reproducible and systematic evaluation of LLM robustness. The actual experiments using this framework and their results are presented in \Cref{ch:Experiment_Setup} and \Cref{ch:Results}.

\section{Enhanced Program Graph Construction}
\label{sec:AST_manipulations}

\subsection{Bieber et al's Program Graphs Framework}
The foundation of our approach builds the \texttt{python\_graphs} library developed by \textcite{Bieber2022}, which provides comprehensive graph representations of Python programs for machine learning applications. The \texttt{python\_graphs} library itself uses the Abstract Syntax Tree (AST) of a program as the ground graph, which it then extends to richer graph representations. The AST captures the syntactic structure of the code, with each node representing a language construct (e.g., function definitions, assignments, loops) and edges reflecting the hierarchical relationships between these constructs. What distinguishes \texttt{python\_graphs} from simple AST representations is its integration of multiple types of program analysis information into a unified graph structure. \\
The library constructs composite program graphs that enrich the AST with additional edge types representing various relationships between code elements. These include control-flow edges (capturing the possible execution paths), data-flow edges (indicating how data values are defined and used throughout the program), lexical edges (representing the ordering of code elements in the source text), and syntactic edges (reflecting the grammatical structure of the program). 
In detail the additional edge types to the "FIELD" edges, that were already existent in the AST \Cref{sec:AST}, are:
\begin{itemize}
    \item \textbf{FIELD}: Represents AST parent-child relationships where the destination is a field of the source AST node
    \item \textbf{NEXT\_SYNTAX}: Connects consecutive syntax elements in top-to-bottom, left-to-right order
    \item \textbf{LAST\_LEXICAL\_USE}: Links a variable use to its previous lexical appearance in the code
    \item \textbf{CFG\_NEXT}: Indicates control flow where the destination statement can execute immediately after the source
    \item \textbf{LAST\_READ}: Connects a variable access to where it was most recently read before that point
    \item \textbf{LAST\_WRITE}: Links a variable access to where it was most recently written/assigned
    \item \textbf{COMPUTED\_FROM}: Connects assignment targets (left-hand side) to the variables used in their computation (right-hand side)
    \item \textbf{CALLS}: Links function call sites to their corresponding function definitions
    \item \textbf{FORMAL\_ARG\_NAME}: Maps arguments at call sites to their corresponding parameters in function definitions
    \item \textbf{RETURNS\_TO}: Connects return statements in functions back to their call sites
\end{itemize}

This overall representation enables machine learning models to capture not only the syntactic structure of the code but also its semantic properties and execution behavior.
One of the key strengths of \texttt{python\_graphs} is its ability to perform static analysis on Python code despite the language's dynamic nature. The library implements control-flow graph construction that handles complex Python features such as exception handling, generator functions, and comprehensions. It also performs data-flow analyses like liveness analysis (determining which variables may be read before their next assignment) and last-access analysis (tracking where variables were last read or written). 
While the original `program\_graph.get\_program\_graph` function provides a comprehensive representation of code with multiple edge types (AST structure, control-flow, data-flow), it was not designed for the specific purpose of generating valid code reorderings. For example all the “NEXT\_\allowbreak SYNTAX” edges didn't suit our purpose. These represent the syntactical order of the statements in code. With those directed edges only one possible reordering would be possible (the current).\\
Instead, we used the `dataflow\_parser.get\_program\_graph` implementation from \textcite{Geisler2023} which are a subset of the full python graphs representing the data dependencies present in a program. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{ProgramGraph.png}
    \caption{\Cref{fig:AST} with additional Program Graph edges}
    \label{fig:ProgramGraph}
\end{figure}

\Cref{fig:ProgramGraph} illustrates the enriched dataflow program graph for our example Python function from \ref{lst:test_function}.

At the top level, we see the AST (Abstract Syntax Tree) hierarchy starting from the \texttt{Module} node, connecting to \texttt{FunctionDef} and its descendants via \texttt{FIELD} edges (shown in black). This represents the basic syntactic structure of the code.

Beyond simple syntax, the graph incorporates several critical edge types that encode semantic information:
\begin{itemize}
    \item Control flow edges (\texttt{CFG\_NEXT}, shown in red) represent possible execution paths through the program
    \item Data flow edges (\texttt{LAST\_WRITE} shown in yellow, COMPUTED\_FROM in blue) track how variables are defined and used
    \item Additional semantic relationships like function calls and argument passing are also encoded
\end{itemize}

These relationships form the constraints that any semantic-preserving transformation must respect. For example, the \texttt{LAST\_WRITE} edges between Name nodes (variable references) ensure that any reordering of statements will preserve the correct order of variable assignments and uses. Similarly, control flow edges ensure that conditionals and their dependent blocks remain properly structured.

These graphs required further modifications like cycle detection and elimination, propagation of dependencies to the right level, adding certain dependency edges that were missing and dropping others that were too rigid for our goal of capturing as many possible reorderings as possible. 

\subsection{Our Extensions for Semantic-Preserving Transformations}

The basic graph representation as shown in \Cref{fig:ProgramGraph} serves as an excellent foundation, but requires significant enhancements to enable precise semantic-preserving transformations. The original representation has several limitations that could lead to incorrect code reorderings: it includes rigid ordering constraints that prevent valid transformations, lacks certain crucial dependencies, and doesn't adequately model relationships between specific language constructs like imports and control blocks.

\subsubsection{Insufficiencies in Standard Representation}

Similarly to the aforementioned "NEXT\_SYNTAX" edges, "LAST\_READ" edges in the original graph often create overly strict dependencies that prevent valid reorderings, particularly around function calls. Also we found that some of the "CFG\_NEXT" edges were a simple replacement in the data flow graphs for the "NEXT\_SYNTAX" edges in the \texttt{python\_graphs}, which again lead to a too rigid graph. \\

Additionally, we found several cases where the standard representation fails to capture important semantic dependencies:

\begin{itemize}
    \item \textbf{Import dependencies}: The original graph doesn't adequately represent dependencies between import statements and where the imported symbols are used.
    \item \textbf{Control block dependencies}: Relationships between control structure nodes (like \texttt{If} statements) and their nested blocks need more precise modeling.
    \item \textbf{Inter-function relationships}: Function definitions and calls require special handling to avoid invalid reorderings.
\end{itemize}

\subsubsection{Enhanced Dependency Tracking}

To address these limitations, we implemented several extension to the graph construction process:

\paragraph{Edge Removal Strategies}
Several edge removal functions eliminate unnecessary constraints while preserving semantic validity:

\begin{itemize}
    \item \texttt{remove\_last\_reads}: Removes certain \texttt{LAST\_READ} edges that are overly restrictive, especially around function calls to standard libraries, which often have no side effects.
    \item \texttt{remove\_cfg\_next\_edges\_between\_functions}: Eliminates \texttt{CFG\_NEXT} edges between functions within the same module or class, allowing independent functions to be reordered.
    \item \texttt{remove\_next\_syntax\_edges\_until\_first\_function\_call}: Modifies \texttt{NEXT\_SYNTAX} edges to enable more flexible statement ordering while maintaining execution semantics.
\end{itemize}

Additionally, our implementation includes cycle detection and elimination mechanisms to ensure the resulting graph is a directed acyclic graph (DAG), which is necessary for valid topological sorting.

\paragraph{Cycle Management}
A crucial aspect of our enhancement is the detection and resolution of cycles in the dependency graph. Using the \texttt{ASTOrder} visitor class, we analyze the order of nodes in the AST and remove any edges that create cycles while ensuring that removal doesn't affect semantic correctness. This process is essential for enabling valid topological sorts of the graph. \\ \\

To illustrate the importance of cycle elimination from the original frame work, consider the follwoing simple code snippet:

\begin{lstlisting}[style=pythonstyle, caption={Python for-loop iterating from 1 to 10 and printing each value. This seemingly simple construct creates cycles in the dependency graph because the loop variable \texttt{i} depends on itself across iterations, requiring special handling in our graph construction.}]
def test_function():
    for i in range(1, 10):
        print(i)
\end{lstlisting}


In for- and while-loops the variable in such a block "depend on themselves" . Throughout different iterations they get overwritten by themselves in the control statement or get last read by functions. While this modelling \texttt{python\_graphs} makes sense for some machine learning applications, loops in our graph make it impossible to topological sort a graph since a DAG is needed for that. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\linewidth]{For_with_loops.png}
    \caption{Program graph with cycles}
    \label{fig:For_with_loops}
\end{figure}


In \Cref{fig:For_with_loops} we notice several loop constructs, that would be probelmatic for our algorithm:
\begin{itemize}
    \item The \texttt{CFG\_NEXT} edges (red) create circular dependencies between the \texttt{Name i} nodes and the \texttt{Expr} node
    \item The \texttt{LAST\_WRITE} edge (yellow) from one \texttt{Name i} to another together with a \texttt{LAST\_READ} edge (green) back form another circle of order 2
    \item Finally the print statement forms a self cycle through a \texttt{LAST\_READ} edge (green)
\end{itemize}

Our \texttt{ASTOrder} visitor class addresses this by traversing the AST in via depth-first and recording the order in which each node was visited. When examining edges, it identifies those that would create cycles by checking if the source node appears after the target node in the AST ordering. The \texttt{creates\_cycle} is then used to check if an edge would contribute to a cycle in the graph.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{For_without_loops.png}
    \caption{Program graph after cycle elimination - now acyclic and ready for topological sorting}
    \label{fig:For_without_loops}
\end{figure}

\Cref{fig:For_without_loops} then shows the graphic after our Cycle Management. It is a DAG now that still keeps all necessary dependencies.


\paragraph{Import Dependency Tracking}

The challenge with import statements in standard program graph representations is that they are often treated as isolated nodes with no explicit connection to where the imported symbols are actually used. This creates a gap in dependency tracking that could lead to invalid reorderings where, for example, a function call using an imported library could be moved before the import statement itself.

Our \texttt{add\_import\_dependencies} implementation addresses this through a two-phase AST analysis process:

\textbf{Phase 1: Import Discovery}
The system first traverses the entire AST to catalog all import statements and the names they introduce into the local namespace. For each import statement, we record:
\begin{itemize}
    \item The graph node ID corresponding to the import statement
    \item The local name(s) that the import makes available (handling both direct imports and aliased imports)
\end{itemize}

\textbf{Phase 2: Usage Tracking}
In the second phase, we examine every \texttt{Name} node in the AST that represents a variable or function being loaded (not stored). For each such usage, we check whether the name corresponds to an imported symbol. If a match is found, we create a \texttt{LAST\_READ} edge from the import statement to the usage location.

The implementation handles several import patterns:

\begin{lstlisting}[style=pythonstyle, caption={Different import patterns that can occur and that need to be covered as dependencies}]
# Direct import
import math
# Creates dependency: math usage -> import math

# Aliased import  
import numpy as np
# Creates dependency: np usage -> import numpy as np

# From import
from collections import defaultdict
# Creates dependency: defaultdict usage -> from collections import defaultdict

# Aliased from import
from math import sqrt as square_root
# Creates dependency: square_root usage -> from math import sqrt as square_root
\end{lstlisting}

To illustrate the impact of our import dependency extension, consider this example code:

\begin{lstlisting}[style=pythonstyle, caption={An example for code with an import statement}]
import math
    
def test_function(a, b):
    x = 5
    y = math.sqrt(x)
    return y
\end{lstlisting}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{Import_without_edge.png}
    \caption{Program graph without import dependency extension}
    \label{fig:Import_without_edge}
\end{figure}

\textbf{Figure~\ref{fig:Import_without_edge}} shows the program graph representation without our import dependency extension. Here, the \texttt{Import} node exists independently, with no explicit connection to where \texttt{math} is used within the function. This could potentially allow invalid reorderings where statements using \texttt{math} could be moved before the import statement.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{Import_with_edge.png}
    \caption{Program graph with import dependency extension showing LAST\_READ edge}
    \label{fig:Import_with_edge}
\end{figure}

\textbf{Figure~\ref{fig:Import_with_edge}} demonstrates the enhanced representation after applying our import dependency tracking. The crucial addition is the green \texttt{LAST\_READ} edge connecting the \texttt{Import} node directly to the \texttt{Name math} node where the imported symbol is used. This edge ensures that any topological sort of the graph will respect the semantic requirement that imports must precede their usage.

Without the import dependency (Figure~\ref{fig:Import_without_edge}), a naive topological sort might produce invalid code such as:

\begin{lstlisting}[style=pythonstyle, caption={Without the necessary dependency the import statement is misplace}, label={lst:import}]
def test_function(a, b):
        x = 5
        y = math.sqrt(x)  # ERROR: 'math' not yet imported
        return y
    
    import math  # Too late!
\end{lstlisting}

With our extension (Figure~\ref{lst:import}), the \texttt{LAST\_READ} edge prevents such invalid reorderings while still permitting semantically valid transformations:

\begin{lstlisting}[style=pythonstyle, caption={Import dependency considerated}]
def test_function(a, b):
    x = 5             # Valid: x is independant from the import statement
    import math
    y = math.sqrt(x)  # Valid: import precedes usage
    return y
\end{lstlisting}

This extension is particularly important for larger programs with multiple imports and complex usage patterns, where the dependency relationships can become quite intricate but must be preserved to maintain program correctness.


\paragraph{Control Block Dependency Propagation}

The \texttt{add\_control\_block\_dependencies} function addresses one of the most complex challenges in maintaining semantic correctness during code transformations: ensuring that control structures and their dependent operations maintain their proper relationships. This function goes beyond simple syntactic dependencies to capture the semantic reality that certain operations must remain associated with their controlling structures.

\textbf{The Core Challenge}

Standard AST and program graph representations often fail to capture the full scope of dependencies created by control structures. While they correctly model the immediate parent-child relationships within control blocks, they miss crucial dependencies between control structures and operations that depend on variables modified within those blocks. Consider this example:

\begin{lstlisting}[style=pythonstyle, caption={A control block (if) with dependencies outside the control block}]
def test_function():
    x = 1
    if x > 0:
        x = 10
    y = 20 + x
\end{lstlisting}    

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{If_without_block.png}
    \caption{Program graph without our block extension}
    \label{fig:If_without_block}
\end{figure}


In this case, the order should be rigid. The assignment of \texttt{x = 1} must occur before being used in the if-statement. Also all the statements after the if-block that use \texttt{x} must come after the whole block

\textbf{Our Enhanced Approach}

Our implementation systematically addresses this by identifying control structure nodes and propagating dependencies from all operations within their scope to the control structure itself. The function handles multiple types of control constructs:

\begin{itemize}
    \item \textbf{Conditional structures} (\texttt{If} statements): All operations within both the \texttt{body} and \texttt{orelse} blocks are analyzed
    \item \textbf{Loop constructs} (\texttt{For}, \texttt{While}): Dependencies are propagated from loop bodies and optional else clauses
    \item \textbf{Exception handling} (\texttt{Try} statements): All operations within try blocks, exception handlers, else clauses, and finally blocks are considered
    \item \textbf{Context managers} (\texttt{With} statements): Operations within the context block are linked to the with statement
    \item \textbf{Assignment operations} (\texttt{Assign}, \texttt{AugAssign}): Target and value dependencies are explicitly tracked
\end{itemize}

\textbf{Dependency Propagation Algorithm}

The algorithm operates through several phases:

\begin{enumerate}
    \item \textbf{Block Identification}: For each control structure node, identify all AST nodes that belong to its various blocks (body, orelse, handlers, etc.)
    \item \textbf{Descendant Discovery}: Use a recursive graph traversal to find all nodes that are transitively dependent on nodes within the control blocks
    \item \textbf{External Dependency Analysis}: For each node within a control block, examine all incoming edges from nodes outside the block
    \item \textbf{Dependency Propagation}: Create new edges from external dependencies directly to the control structure node, ensuring that any operation depending on the control block must wait for the entire structure to be positioned
\end{enumerate}

For the technical implementation we first get all the descendants of a node via a graph traversal:

\newpage
\begin{lstlisting}[style=pythonstyle, caption={Tracks descendants recursively}]
memo_descendants = {}
def get_descendants(node_id, current_path_visited=None):
    if node_id in memo_descendants:
        return memo_descendants[node_id].copy() # Return a copy

    if current_path_visited is None:
        current_path_visited = set()
    if node_id in current_path_visited: # Cycle detected in current traversal path
        return set()

    current_path_visited.add(node_id)
    descendants = set()
    for successor_id in adj.get(node_id, set()):
        descendants.add(successor_id)
        descendants.update(successor_id, current_path_visited.copy())
    current_path_visited.remove(node_id) # Backtrack

    memo_descendants[node_id] = descendants.copy()
    return descendants
\end{lstlisting}

This ensures that all dependencies within control blocks are properly identified. Then we can propagate the dependency to the right level.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{If_with_block.png}
    \caption{Program graph with our Block extension}
    \label{fig:If_with_block}
\end{figure}


Figure \Cref{fig:If_without_block} shows the program graph for our example before applying control block dependency propagation. Here, operations around the if statement might be incorrectly reordered potentially changing program semantics.

Figure~\ref{fig:If_with_block} demonstrates the enhanced representation after applying our control block dependency enhancement. The new dependency edges ensure that any operation depending on variables modified within the if block properly depends on the entire conditional structure.

\paragraph{Sequential Dependencies for Jump Statements}
A particularly subtle challenge in code reordering involves control flow statements like \texttt{break} and \texttt{continue} within loops. While these statements have explicit control flow semantics, the standard program graph representation may not adequately capture that all preceding statements within the same syntactic block must execute before the jump. Without proper dependencies, a topological sort might incorrectly reorder statements after a \texttt{break} or \texttt{continue} to appear before it, fundamentally altering program behavior. Our \texttt{SequentialJumpDependencyVisitor} addresses this by traversing the AST and adding \texttt{CFG\_NEXT} edges from every statement preceding a jump statement within the same block to the jump itself. As illustrated in Figures [ref to your images], this ensures that expressions like \texttt{print(i)} that appear before a \texttt{break} statement maintain their relative ordering. This proves essential for preserving the intended control flow - for instance, ensuring that diagnostic print statements execute before loop termination, or that state modifications complete before a \texttt{continue} skips to the next iteration. The visitor recursively processes all statement-containing nodes (\texttt{FunctionDef}, \texttt{If}, \texttt{For}, \texttt{While}, \texttt{Try}, \texttt{With}), guaranteeing that jump statements at any nesting level maintain their proper sequential dependencies. \\ \\
The following code serves as a simple example:

\begin{lstlisting}[style=pythonstyle, caption={The break-statement needs to come after the print statement in any valid reordering}]
def test_function():
    for i in range (1,10):
        print(i)
        if i == 1:
            print("Hallo")
            break
\end{lstlisting}

In order to preserve the semantics of this code snippet, the order of the \texttt{print} statement needs to stay before the \texttt{break} statement. In the original program graph, there was no edge between the two statements that would capture the dependency. However, with our graph modification we added \texttt{CFG\_NEXT} edges pointing to \texttt{break} and \texttt{continue} statements so that these statements always come last in within a block.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.3\linewidth]{If_without_break.png}
    \caption{Program graph without jump statement dependencies}
    \label{fig:If_without_break}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.3\linewidth]{If_with_break.png}
    \caption{Program graph with added CFG\_NEXT edges ensuring statements precede break}
    \label{If_with_break}
\end{figure}


\subsubsection{Correcting LAST\_READ Edge Handling in Dataflow Analysis}

Our analysis of the original \texttt{python\_graphs} implementation revealed a critical oversight in the handling of \texttt{LAST\_READ} edges during dataflow analysis. The original implementation in \texttt{dataflow\_parser.py} failed to properly track and create edges for read operations, focusing primarily on write operations. This limitation prevented the accurate modeling of data dependencies where variables are read but not modified. These can be simple \texttt{print} or \texttt{logger }statements or functions that receive objects as arguments.

Our modifications to address this issue include:

\begin{enumerate}
    \item \textbf{Read Identifier Tracking}: We took the already implemented write identifiers and applied the logic to also handle read identifiers properly in the main dataflow analysis loop.
    \begin{lstlisting}[style=pythonstyle, caption={Adjustment in get\_program\_graph}]
    read_identifier = instruction_module.access_identifier(
        access_name, "read"
    )
    \end{lstlisting}
    This ensures that both read and write accesses are properly identified and tracked throughout the analysis.
    \item \textbf{Conditional LAST\_READ Edge Creation}: In the \texttt{CustomControlFlowWalker.generic\_visit} method, we added logic to handle \texttt{LAST\_READ} edges based on context:
    \newpage
    \begin{lstlisting}[style=pythonstyle, caption={Adjustment in CustomControlFlowWalker.generic\_visit}]
    if edge.type == pb.EdgeType.LAST_READ:
    # Check if the read is part of a whitelisted function call
    if any([isinstance(parent, ast.gast.Call) and 
            isinstance(parent.func, ast.gast.Attribute) and 
            (parent.func.attr in white_list_builtin or 
             parent.func.attr in white_list_numpy) 
            for parent in parents]):
        pass  # Skip adding dependency for pure functions
    else:
        node_depends_on.add(edge.id1)
    \end{lstlisting}    
    This approach distinguishes between reads that create true dependencies and those that can be safely ignored for reordering purposes.

    \item \textbf{Access Guard Condition}: We added a safety check to prevent errors when processing nodes without any accesses:

    \begin{lstlisting}[style=pythonstyle, caption={Another adjustment in get\_program\_graph}]
    if len(control_flow_node.instruction.accesses) > 0:
    # Process accesses
    \end{lstlisting}    
    This prevents the analysis from failing on nodes that do not involve any variable accesses.
\end{enumerate}

These corrections ensure that our dependency graph accurately represents both read and write relationships between program elements, enabling more precise identification of valid code transformations while maintaining semantic correctness. 


\subsubsection{Impact on Transformation Space}

Our enhanced dependency tracking significantly expands the space of valid code transformations compared to the original representation. By removing unnecessary constraints and adding precisely targeted dependencies, we enable a much richer set of semantically equivalent code variants. A single program might have dozens or even hundreds of valid reorderings under our enhanced representation, while the standard representation would often permit only the original ordering.

This enriched transformation space is crucial for our research on LLM robustness, as it allows us to systematically explore how models respond to surface variations that preserve underlying program semantics. The expanded set of valid reorderings also increases our ability to discover potential adversarial examples that reveal weaknesses in LLM code understanding.

\section{Code reorderings through sorts}
\label{sec:Code_reorderings_through_sorts}

With our modified version of a program graph and our topological sort implementation, we create the following pipeline to get valid reordered code:

\begin{enumerate}
    \item \textbf{Get the graph from code}: Following the previous chapter, we create a program graph from dataflow\_parser and apply all of our transformations to it
    \item \textbf{NetworkX Graph Construction}: We convert our enhanced program graph into a NetworkX directed graph, preserving all nodes and edges:
    \begin{lstlisting}[style=pythonstyle, caption={Code snippet showing the conversion from our custom program graph representation to NetworkX format. This conversion enables the use of NetworkX's graph algorithms, particularly cycle detection and topological sorting, which are essential for identifying valid code reorderings.}]
    nx_graph = nx.DiGraph()
    nx_graph.add_nodes_from(graph.nodes.keys())
    edges = [(edge.id1, edge.id2) for edge in graph.edges]
    nx_graph.add_edges_from(edges)
    \end{lstlisting}
    This library has many useful tools for us like checking if the graph is a DAG or detecting cycles.

    \item \textbf{Topological Sort Generation}: We apply our topologcial sort to this graph. In the next section we also describe a second approach besides Random Topological Sorts, Neighborhood Topological Sorts, which refines our adversarial attack in some cases.

    \item \textbf{AST Reconstruction}: The \texttt{ASTReconstructor} class rebuilds the AST according to the new topological order, visiting each node type and reordering their bodies

    \item \textbf{Code Generation}: The reconstructed AST is unparsed back into Python source code using \texttt{astunparse}
\end{enumerate}

\subsection{AST Reconstruction}

The reordering happens in the \texttt{ASTReconstructor} class, which traverses the AST in a depth-first-search and reorders statement bodies based on the computed topological sort. The algorithm handles different AST node types through specialized visitor methods:

\begin{lstlisting}[style=pythonstyle, caption={The ASTReconstructor reordering statements by traversing the nodes of a program graph}]
class ASTReconstructor(ast.NodeTransformer):
    def __init__(self, graph, topo_sort, show_reorderings=False):
        self.graph = graph
        self.topo_sort = topo_sort
        self.ast_map = {node_id: graph.nodes[node_id].ast_node 
                       for node_id in topo_sort}
    
    def reorder_body(self, body):
        # Map AST nodes to their graph node IDs
        ast_to_node_id = {id(ast_node): node_id 
                         for node_id, ast_node in self.ast_map.items()}
        
        # Identify which body statements are in our subgraph
        body_ids = []
        for stmt in body:
            stmt_id = id(stmt)
            if stmt_id in ast_to_node_id:
                body_ids.append(ast_to_node_id[stmt_id])
        
        id_set = set(body_ids)
        
        # Reorder based on topological sort
        reordered_body = [
            self.ast_map[node_id]
            for node_id in self.topo_sort
            if node_id in id_set
        ]
        return reordered_body
\end{lstlisting}
\subsubsection{Handling blocks}
For control structures like \texttt{if} statements, \texttt{for} loops, \texttt{while} loops and so on, we recursively reorder their bodies:

\begin{lstlisting}[style=pythonstyle, caption={The recursive handling of blocks, here the if-blocks}]
def visit_If(self, node):
        self.generic_visit(node)
        node.body = self.reorder_body(node.body)
        node.orelse = self.reorder_body(node.orelse)
        return node
\end{lstlisting}
\subsubsection{Binary Operation Reordering}
We also support the random reordering of commutative binary operations, recognizing that expressions like \texttt{a + b} and \texttt{b + a} are semantically equivalent. Similarly, we include \texttt{multiplication} and the \texttt{bit-wise} operations Or, Xor and And. For Addition we have to account for the addition of Strings, which is not commutative thus we exclude them from the swapping. That also includes the addition of functions like \texttt{str()}, which produce Strings. So our final operations looks like this:

\begin{lstlisting}[style=pythonstyle, caption={Swapping of commutative binary operations}]
def visit_BinOp(self, node: gast.BinOp):
        self.generic_visit(node)
        op_name = node.op.__class__.__name__
        can_swap = False
        if op_name in ("Mult", "BitOr", "BitXor", "BitAnd"): 
            can_swap = True
        elif op_name == "Add":
            is_left_str = self._is_likely_string_producing_node(node.left)
            is_right_str = self._is_likely_string_producing_node(node.right)
            if not (is_left_str or is_right_str):
                can_swap = True 
        if can_swap:
            if random.choice([True, False]):
                node.left, node.right = node.right, node.left
        return node
\end{lstlisting}
The implementation for the commutative compare operations \texttt{Equals} and \texttt{NotEquals} is analog. 

\subsection{Validation and Correctness}

Throughout the reordering process, we maintain semantical correctness by respecting all dependency edeges from the original graph in the new ordering and by making sure that control flow structures remain properly nested. \\

The \texttt{is\_valid\_topological\_sort} function verifies that our generated ordering respects all dependencies:

\newpage
\begin{lstlisting}[style=pythonstyle, caption={Sanity check if the created neighborhood sort is still a valid topological sort}]
def is_valid_topological_sort(graph, sort_order):
        position = {node: idx for idx, node in enumerate(sort_order)}
        
        # For each edge (u,v), ensure u comes before v
        for u, v in graph.edges():
            if position[u] >= position[v]:
                return False
        return True
\end{lstlisting}
This validation step ensures that every transformation we generate preserves the semantic meaning of the original program while exploring the space of syntactically different but semantically equivalent variants.

On a higher level, this is how our Semantic-Preserving Code Perturbation works:

\begin{algorithm}
\caption{Semantic-Preserving Code Perturbation}
\begin{algorithmic}[1]
\State \textbf{Input:} Source code $C$
\State \textbf{Output:} Perturbed code $C'$ with same semantics
\State $G \gets$ BuildProgramGraph($C$) 
\State $G' \gets$ ApplyDependencyRefinements($G$)
\State $T \gets$ TopologicalSort($G'$)
\State $A \gets$ GraphToAST($G'$, $T$) \Comment{Reconstruct AST from graph and new topological order}
\State $C' \gets$ ASTunparse($A$)
\State \textbf{return} $C'$
\end{algorithmic}
\end{algorithm}

\section{Neighborhood sampling for better adversarial examples}
\label{sec:Neighborhood_sampling}
In some cases where the scoring of the LLMs output is measured on a finer scale, neighborhood sampling can refine our search for adversarial examples. Assuming that challenging input to the LLM is closely related to other challenging input, we want select an already bad sample and try to make it even worse by only alternating it a little.

\subsection{The Neighborhood Topological Sort Algorithm}

Our \texttt{neighborhood\_topological\_sort} function generates new orderings that are similar to a reference ordering. This enables us to 

\newpage
\begin{lstlisting}[style=pythonstyle, caption={Neighborhood topological sort that gets as an input the current topological sort and the probability of varying from that}]
def neighborhood_topological_sort(graph, current_sort, change_probability=0.3):
    # Create a mapping from node to position in the current sort
    position_map = {node: idx for idx, node in enumerate(current_sort)}
    
    graph_copy = graph.copy()
    new_sort = []
    sources = deque([node for node in graph_copy.nodes 
                    if graph_copy.in_degree(node) == 0])
    
    while sources:
        # Decide whether to follow current_sort's preference or choose randomly
        if len(sources) > 1 and random.random() < change_probability:
            # Choose randomly from available sources
            random_index = random.randint(0, len(sources) - 1)
            node = sources[random_index]
        else:
            # Choose the node that appears earliest in current_sort
            node = min(sources, key=lambda n: position_map.get(n, float('inf')))
        
        # Continue with standard topological sort procedure
\end{lstlisting}
The key lies in the selection strategy when multiple nodes have no incoming edges. With probability $(1 - \text{change\_probability})$, we select the node that appeared earliest in the reference ordering, preserving the existing structure. Otherwise, we select randomly, introducing controlled variation.




\chapter{Experiment Setup}
\label{ch:Experiment_Setup}
\section{Datasets}

Identifying an appropriate benchmark for the research question presented significant challenges. Most code generation benchmarks give simple instructions in their prompts to make the model create code. However, some code is already needed in the prompt that the aforementioned perturbation can be applied to. Additionally, tasks like bug detection or code repair are not tasks that can be for this purpose since perturbations only works on valid Python code. \\

The fundamental limitation of existing benchmarks stems from their design philosophy. Popular benchmarks like HumanEval \cite{Chen2021} and MBPP \cite{Austin2021} are designed for \textit{code generation from natural language descriptions}, where the prompt contains only a function signature and a docstring describing what to implement. For example, a typical HumanEval prompt looks like:

\begin{lstlisting}[style=pythonstyle, caption={An example of a HumanEval prompt}]
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """ Check if in given list of numbers, are any two numbers closer to each other than
    given threshold.
    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)
    False
    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
    True
    """
\end{lstlisting}
This format is fundamentally incompatible with the proposed research approach, which requires \textit{existing code in the prompt} that we can systematically perturb through statement reordering and commutative operation swapping. We cannot apply semantic-preserving transformations to code that doesn't exist yet.

Similarly, code repair tasks like it is a part of CodeScope \cite{Yan2023}  present a different but equally problematic limitation: they contain \textit{syntactically or semantically incorrect code} that needs to be fixed. Our graph-based perturbation approach, as described in Chapter~\ref{ch:Our_approach}, requires syntactically valid Python code to construct proper ASTs and dependency graphs. Attempting to apply topological sorting to buggy code would either fail during parsing or produce meaningless results.
Suitable benchmarks must provide substantial amounts of complete, syntactically valid code in the prompt while asking models to perform tasks that demonstrate code understanding rather than pure generation. Furthermore, it should allow us to measure how performance changes when we apply semantic-preserving perturbations to the prompt code. For this a benchmark should include objective evaluation metrics, either through unit tests or reliable scoring methods.


This led us to identify two suitable benchmark categories that meet these criteria: code completion tasks (where the model must understand and complete partial code) and fill-in-the-middle tasks (where the model must generate code that fits within an existing context). We adapted datasets from these categories to create evaluation setups that allow us to systematically test LLM robustness to our semantic-preserving transformations.

\section{CodeScope - Reconstructing Code Summary Data}
\label{sec:CodeScop}
CodeScope \cite{Yan2023} is a comprehensive multilingual benchmark that evaluates LLMs across 43 programming languages and 8 different coding tasks. These include Code Summarization, Code Smell, Code Review, Automated Testing, Program Synthesis, Code Translation, Code Repair, Code Optimization. We focused on its code summarization dataset, which crucially provides complete, executable Python functions paired with human-written summaries. \\
However, the original summarization task (generating a description from code) only had evluation metrics that are not so meaningful (comparing Rouge or Bleu Scores of the generated summarization string and a ground truth string). 
We therefore inverted the task to create a code completion benchmark. Our transformation process works as follows:

\subsection{Dataset Transformation}
We developed a systematic approach to convert complete functions into incomplete ones while preserving enough structure for meaningful evaluation:

\begin{lstlisting}[style=pythonstyle, caption={Function that substitutes all the existing function bodies of a module with pass statements}]
class IncompleteTransformer(ast.NodeTransformer):
    def __init__(self):
        super().__init__()
        self.modified = False

    def visit_FunctionDef(self, node):
        self.generic_visit(node)
        # Check if the function has a docstring as the first statement
        if (node.body and isinstance(node.body[0], ast.Expr) and
            isinstance(node.body[0].value, ast.Constant) and
            isinstance(node.body[0].value.value, str)):
            docstring = node.body[0]
            node.body = [docstring, ast.Pass()]
        else:
            node.body = [ast.Pass()]
        # Mark that a function body was substituted with a pass
        self.modified = True
        return node
\end{lstlisting}
This transformer replaces all function bodies with \texttt{pass} statements while preserving docstrings, creating a skeleton that the LLM must complete based on the human summarization and the surrounding code context. Importantly, this transformation maintains the overall code structure, allowing us to apply our perturbations to the context code while keeping the completion target consistent.

From the original CodeScope code summarization dataset we successfully transform 46 samples that were written in Python and have at least at least one function body could be replaced. Out of these 46 samples we then continue our evaluation with 37 samples that have enough perturbation potential.

\subsection{Evaluation Task Design}
Our modified task presents the LLM with a comprehensive code completion scenario consisting of three key components. First, the model receives a human-written summarization that describes the code's functionality in natural language. Second, it encounters an incomplete code skeleton containing function signatures with \texttt{pass} statements that need to be replaced with actual implementations. Third, the model has access to the surrounding code context, including imports, other functions, and classes that provide essential contextual information. 
The model must generate complete implementations for all incomplete functions. Crucially, the surrounding context code can be perturbed using our semantic-preserving transformations, allowing us to test whether models truly understand code semantics or merely memorize syntactic patterns.


\subsection{LLM as a judge}
Given our generation setup, we also need a setup to evaluate the quality of an LLM's generations. Since we don't have unit tests to assess the code's functionality, we have implemented an LLM-as-a-judge approach. This method has gained growing attention in recent research \cite{Zheng2023, GU2025} and is justified by LLM's growing ability to think an reason like a human expert would. Therefore LLM-as-a-judge is often used in scenarios like ours as a cost-effective solution that can be effortlessly scaled to meet increasing evaluation demands in experiments. \\

Our evaluation pipeline uses a two-stage approach:
\paragraph{Stage 1: Code Completion}
In the first stage of our evaluation pipeline, the completion model receives the incomplete code along with the summarization and generates the missing implementations. We validate these completions through two criteria of which one has to be fulfilled so that the completion is counted as valid: The number of \texttt{pass} statements has decreased or the code length has increased by at least 20\%. Additionally we confirm that the generated code is syntactically valid.
\paragraph{Stage 2: Quality Judgement}
A separate judge model evaluates the completed code against the ground truth implementation and human summarization using a 5-point scale:

\begin{lstlisting}[style=pythonstyle, caption={Prompt template for the judge model}]
"1 - Very poor: The generated code completely fails to meet the requirements.
2 - Poor: The code shows some attempt to address the requirements but has major issues.
3 - Average: The candidate meets the basic requirements.
4 - Good: The generated code is high-quality-it mostly meets the requirements and follows best practices.
5 - Excellent: The code fulfills all the requirements and is functionally equivalent to the ground truth."
\end{lstlisting}
This structured evaluation allows us to quantify how code perturbations affect completion quality. The judge model receives the complete ground truth code, the human summarization, and the generated completion, providing a comprehensive context for evaluation.

\subsection{Justification for LLM-as-a-Judge}

The literature establishes multiple justifications for LLM-as-a-Judge as a valid approach. 

\begin{enumerate}
\item \textbf{Scalability}: Manual evaluation of thousands of code completions across multiple perturbations would be prohibitively expensive and time-consuming.
\item \textbf{Consistency}: Unlike human evaluators who may suffer from fatigue or subjective biases, the LLM judge applies consistent criteria across all evaluations.

\item \textbf{Relative Comparison}: Our primary interest is in the \textit{relative} performance drop between original and perturbed code, rather than absolute quality scores. Even if the judge has systematic biases, these should affect both conditions equally.
\end{enumerate}

Besides these theoretical results, we also calculated an agreement on a small subsample. We let a model (in this case Gemma 3-4B) make the completions and let our judgement model (Gemma 3-12B) make judgements. On 35 samples we also calculated a Human score besides the given score by the LLM-as-a-judge.
\textcite{GU2025} define the agreement as 
\begin{equation}
    \text{Agreement} = \frac{\sum_{i \in D} I(S_{LLM} = S_{Human})}{||D||}
\end{equation}
where $D$ is the dataset, $S_{LLM}$ and $S_{Human}$ is the evaluation result of LLM evaluator and human judge respectively. \\
For our case though, we calculate a Pearson correlation, since our evaluation is on a scale. We don't just want to capture exact agreements but also slight agreements. If the human rating is a 1, a 2 by the LLM is still more agreement than a 5 would be. In this setup, we got a correlation of \textbf{85.5\%}, which is quite high and further justifies our approach. \\
Overall, to ensure reliability, we use different models for completion but always the same for judging judging (e.g., different Llama and Gemma models for completion but always Gemma-3-12B for judging).

\subsection{Experimental Protocol}
Our experimental protocol for the CodeScope dataset follows these steps:
\begin{enumerate}
\item \textbf{Baseline Evaluation}: Generate completions for the original (unperturbed) incomplete code and obtain judge scores
\item \textbf{Perturbation Generation}: Apply our graph-based reordering algorithm to generate multiple semantically equivalent versions of the context code
\item \textbf{Adversarial Search}: Use our neighborhood search algorithm to find perturbations that maximize performance degradation
\item \textbf{Statistical Analysis}: Compare score distributions between original and perturbed versions to quantify robustness
\end{enumerate}
This setup allows us to systematically evaluate whether LLMs truly understand code structure or are overly sensitive to syntactic variations that preserve semantic meaning.\\
Our adversarial search in step three of our pipeline follows a two phase search strategy. It combines random exploration with targeted refinement:
\begin{enumerate}
    \item \textbf{Initial Random Sampling}: We begin by exploring the space of semantic-preserving transformations through random sampling. Through non-deterministic topological sorting we draw an initial batch (Default 20 samples) that we then evaluate.
    \item \textbf{Adaptive Neighborhood Search}: Starting from the worst-scoring sample found in Phase 1, we perform iterative refinement using neighborhood perturbations. For each iteration (Default 4), we draw a batch (Default 20 samples) in the neighborhood of the worst-scoring sample from the previous iteration.
    \item \textbf{Refinement}: Depending on whether we found a worse-scoring sample or not, the change probability is decreased or increased to adapt the field of our search.
\end{enumerate}



\section{Safim - Using 'Fill in the middle' data with unit tests}

SAFIM (Syntax-Aware Code Fill-in-the-Middlebeg) \cite{Gong2024} provides a comprehensive benchmark for evaluating code completion models on Fill-in-the-Middle (FIM) tasks. Unlike traditional left-to-right code generation benchmarks, SAFIM tests models' ability to understand and complete code within existing contexts, making it ideal for our perturbation-based robustness evaluation.

\subsection{Dataset Overview}

The SAFIM benchmark contains multiple datasets that target different code completion scenarios, providing comprehensive coverage of real-world programming tasks. The benchmark includes block completion tasks where models must complete entire code blocks such as function bodies and loop contents, control structure completion requiring the completion of control flow statements like if/else branches and loop conditions, and API call completion where models must complete function calls with appropriate arguments. Each sample in SAFIM is structured with four essential components: an \texttt{eval\_prompt} containing incomplete code with a \texttt{{{completion}}} placeholder marking where code should be inserted, the \texttt{ground\_truth} representing the correct code to fill the placeholder, executable \texttt{unit\_tests} that validate functional correctness of the completion, and a unique \texttt{task\_id} for sample identification. This structure perfectly aligns with our requirements by providing complete, syntactically valid code in the prompt that can be parsed and perturbed, while maintaining objective evaluation through unit tests.

\subsection{Evaluation Methodology}

Our evaluation process for SAFIM follows a comprehensive two-phase approach designed to assess both baseline performance and adversarial robustness. In the baseline evaluation phase, we first evaluate each model's performance on the original, unperturbed code by applying the appropriate prompt template based on the completion type, generating the completion using the target LLM, replacing the \texttt{{{completion}}} placeholder with the generated code, and executing unit tests to determine functional correctness. For samples where the model produces correct completions, we proceed to the adversarial evaluation phase. This phase involves constructing the full code by replacing the placeholder with the ground truth, applying our perturbation algorithm to reorder statements while preserving semantics, replacing the ground truth with the placeholder in the reordered code, and testing whether the model can still produce a correct completion when presented with the perturbed prompt. This two-phase methodology enables us to identify models that not only perform well on standard benchmarks but also demonstrate robustness to semantic-preserving transformations.

\subsection{Implementation Details}

Our implementation extends the original SAFIM evaluation framework with adversarial capabilities:
\begin{lstlisting}[style=pythonstyle, caption={Adversarial Example Generation for SAFIM}]
def generate_adversarial_reordering(eval_prompt: str, ground_truth: str) -> Optional[str]:
    # Create complete code for analysis
    full_code = eval_prompt.replace("{{completion}}", ground_truth)
    
    # Apply perturbation algorithm
    reordered_full_code, _ = perturbation(
        code=full_code,
        ground_truth_code_str=ground_truth,
        apply_perturbation=True
    )
    
    # Replace ground truth with placeholder marker
    unparsed_placeholder = astunparse.unparse(
        gast.Expr(value=gast.Constant(value=PLACEHOLDER_STR))
    ).strip()
    
    # Create adversarial prompt
    reordered_incomplete = reordered_full_code.replace(
        unparsed_placeholder, "{{completion}}", 1
    )
    
    return reordered_incomplete
\end{lstlisting}
A key challenge in applying perturbations to fill-in-the-middle tasks is that the incomplete code (with \texttt{\{\{completion\}\}} placeholders) is not syntactically valid and thus cannot be parsed into an AST. \\
We developed a novel approach that tracks ground truth nodes through the perturbation process. 
The perturbation function identifies which AST nodes correspond to the ground truth. 
During AST reconstruction, we replace identified ground truth nodes with a unique placeholder.
After perturbation, we restore the original \texttt{\{\{completion\}\}} placeholder.

This methodology ensures that the ground truth code is never reordered internally (preserving its semantic integrity), while the surrounding context can be freely reordered according to dependency constraints. The use of a UUID-based placeholder string (\texttt{PERTURBATION\_PLACEHOLDER\_STR\_RAW}) guarantees no accidental matches with actual code content.


\subsubsection{Experimental Configuration Details}

For our experiments, we specifically utilized the SAFIM block completion dataset, which contains 8,781 examples requiring models to complete entire code blocks such as function bodies and loop contents. In comparison, we also ran our experiments on the API completion subset, but due to its simplicity and limitation on perturbations all models stayed robust. Other than the block subset, the API subset also does not contain any unit tests. \\
This subset was chosen as it best aligns with our semantic-preserving transformation approach—block-level completions provide sufficient code structure for meaningful statement reordering while maintaining clear correctness criteria through unit tests.

Our evaluation pipeline employs SAFIM's built-in post-processing mechanisms, including:
\begin{itemize}
    \item \textbf{Syntax-aware truncation}: Following SAFIM's iterative truncation algorithm to extract valid code blocks from model outputs
    \item \textbf{AST validation}: Ensuring generated completions form valid subtrees that integrate correctly with the surrounding code structure  
    \item \textbf{Timeout handling}: 10-second execution limits per test case to handle potential infinite loops
\end{itemize}

We use the default SAFIM prompt template for block completion tasks, which provides the incomplete code with \texttt{\{\{completion\}\}} markers. Models generate at temperature 0.2 following SAFIM conventions, with deterministic sampling (do\_sample=False) in comparative experiments to isolate the effects of our perturbations from sampling randomness.

\subsection{Test Execution and Validation}

SAFIM's unit test framework provides robust validation of functional correctness:
\newpage

\begin{lstlisting}[style=pythonstyle, caption={Unit Test Execution}]
def run_python_tests(code_string: str, unit_tests: list) -> Tuple[str, bool, int, int, int]:
    passed_count = 0
    failed_count = 0
    
    for test_case in unit_tests:
        test_input = test_case.get("input", "")
        expected_outputs = test_case.get("output", [])
        
        # Execute code with test input
        process = subprocess.run(
            ["python", temp_file_path],
            input=str(test_input),
            capture_output=True,
            timeout=10
        )
        
        # Validate output
        if process.returncode == 0:
            actual_output = process.stdout.strip().splitlines()
            if actual_output == expected_outputs:
                passed_count += 1
            else:
                failed_count += 1
                
    return status, passed_count == len(unit_tests), passed_count, failed_count
\end{lstlisting}
This execution-based evaluation ensures that we measure true functional correctness rather than syntactic similarity, making it impossible for models to rely on memorized patterns.

\subsection{Adversarial Search Strategy}

For each sample where the model initially succeeds, we employ an iterative search for adversarial examples (\Cref{alg:safim-adversarial}).


\begin{algorithm}[htbp]
\caption{SAFIM Adversarial Example Search}
\label{alg:safim-adversarial}
\begin{algorithmic}[1]
\Require Original prompt $P$, Ground truth $G$, Model $M$, Tests $T$
\Ensure Adversarial prompt $P'$ or failure indication
\State $\text{max\_attempts} \gets 10$
\For{$i = 1$ \textbf{to} $\text{max\_attempts}$}
    \State $P' \gets \text{generateAdversarialReordering}(P, G)$
    \If{$P' = \text{None}$}
        \State \textbf{continue} \Comment{Perturbation failed}
    \EndIf
    \State $C' \gets M(P')$ \Comment{Get model completion}
    \State $\text{code} \gets P.\text{replace}(\text{``\{\{completion\}\}''}, C')$
    \State $\text{passed}, \text{stats} \gets \text{runTests}(\text{code}, T)$
    \If{not $\text{passed}$ \textbf{and} $C' \neq G$}
        \State \Return $P'$ \Comment{Found adversarial example}
    \EndIf
\EndFor
\State \Return \text{None} \Comment{No adversarial example found}
\end{algorithmic}
\end{algorithm}

\subsection{Evaluation Metrics}

We track several metrics to assess model robustness:

\begin{itemize}
    \item \textbf{Original Pass Rate:} Percentage of samples correctly completed without perturbation
    \item \textbf{Vulnerability Rate:} Among originally passing samples, percentage where adversarial examples were found
    \item \textbf{Robustness Rate:} Percentage of samples resistant to all perturbation attempts
    \item \textbf{Test-Level Metrics:} Granular analysis of which specific tests fail under perturbation
\end{itemize}

The SAFIM dataset's combination of realistic code contexts, executable validation, and diverse completion types makes it an ideal benchmark for evaluating LLM robustness to semantic-preserving perturbations.


\chapter{Results}
\label{ch:Results}

\section{Evaluation using LLM as a judge}

This chapter presents the empirical results of our experiments on LLM robustness to semantic-preserving code transformations. We evaluate multiple language models using our adversarial reordering framework, with Gemma 3 12B serving as the judge model for all experiments.

\subsection{Experimental Setup Summary}

Our evaluation tested four state-of-the-art language models on \textbf{37} carefully selected code samples from the code summarization dataset. We tested models across different scales, including Gemma 3 in both 4B and 12B parameter variants, alongside Llama 3.1 8B and the more compact Llama 3.2 3B. This selection provides insights into how model size and architecture influence robustness to semantic-preserving transformations.

For consistent evaluation across all models, we configured our neighborhood sampling algorithm with an initial exploration phase of 20 samples, followed by four refinement iterations, each generating 20 neighborhood samples. The algorithm began with a change probability of 0.3, which adapted dynamically throughout the iterations based on the success of finding more vulnerable examples. This adaptive approach balances exploration of the transformation space with exploitation of promising perturbation directions.

\subsection{Vulnerability to Adversarial Code Reordering}

\subsubsection{Overall Performance Degradation}

Table \ref{tab:overall-performance} summarizes the impact of adversarial reordering on each model's performance. The results indicate that all tested models in our experiments showed measurable vulnerability to semantic-preserving transformations.

\begin{table}[h]
\centering
\caption{Overall performance comparison across models}
\label{tab:overall-performance}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Orig Avg} & \textbf{Adv Avg} & \textbf{Avg Drop} & \textbf{\% Samples that got worse} \\
\midrule
Gemma 3 4B & 2.70 & 2.46 & 0.24 & 27.0\% \\
Gemma 3 12B & 3.25 & 2.64 & 0.61 & 52.7\% \\
Llama 3.1 8B & 2.95 & 2.38 & 0.57 & 43.2\% \\
Llama 3.2 3B & 2.95 & 2.32 & 0.62 & 51.4\% \\
\bottomrule
\end{tabular}
\end{table}

The observed vulnerability rates reveal several unexpected patterns that warrant closer examination. The inverse relationship between model size and robustness particularly challenges conventional assumptions about scaling laws in machine learning. While Gemma 3 12B represents the largest model in our evaluation set with presumably superior benchmark performance, its 52.7\% vulnerability rate suggests that increased parameter count may actually expand the attack surface for adversarial perturbations.

This phenomenon could stem from larger models developing more complex and fragile internal representations of code syntax. During training on massive code corpora, these models may overfit to specific syntactic patterns prevalent in their training data. The additional parameters allow them to memorize relationships between code structure and expected outputs, but this memorization becomes a liability when faced with valid but uncommon syntactic arrangements.

The relatively better performance of Gemma 3 4B (only 27.0\% of the tested samples showed vulnerability) suggests that smaller models may be forced to learn more robust, generalizable features due to their limited capacity. This capacity constraint could act as an implicit regularizer, preventing the model from relying too heavily on superficial syntactic cues. This interpretation aligns with recent findings in other domains where model compression sometimes improves robustness to adversarial examples.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{Overview_model_performance.png}
    \caption{Performance comparison of four LLM models (Gemma 3 4B, Gemma 3 12B, Llama 3.1 8B, Llama 3.2 3B) showing original vs adversarial average scores (top left), absolute performance drop (top right), percentage of samples that degraded under adversarial perturbation (bottom left), and the relationship between performance drop and average drop (bottom right). All evaluations used Gemma 3 12B as the judge model.}
    \label{fig:Overview_model_performance}
\end{figure}


\subsection{Characteristics of Effective Adversarial Transformations}

While a few samples only show two or four possible reorderings, the average number of unique perturbations per sample is 68 for one model. 15 samples here reach the maximum number of 100 unique perturbations.

\subsection{Case Study: High-Impact Transformations}

Our analysis of specific code samples reveals distinct patterns in how structural complexity influences vulnerability to reordering. We examine three representative examples that span the spectrum of transformation potential and impact.

\subsubsection{Example 1: Stack Implementation (Sample 4672)}

The stack implementation demonstrates remarkable stability under our perturbation framework. With both maximum and minimum scores holding steady at 4, this sample showed complete immunity to reordering attacks despite our adversarial search. The limited transformation space of only 4 unique valid reorderings didn't leave a lot of room for our adversarial attack setup. This example illustrates how simple code without many possibilities for reorderings stays robust through all tested models.

\subsubsection{Example 2: Dijkstra's Algorithm (Sample 4678)}

Dijkstra's shortest path algorithm presents a contrasting vulnerability profile. The model's performance degraded from a maximum score of 3 to a minimum of 2 under adversarial reordering. With up to 89 unique valid reorderings, this implementation contains numerous initialization steps, auxiliary data structure updates, and independent calculations that can be rearranged while preserving algorithmic correctness. The performance drop suggests that models have learned to associate specific syntactic patterns with this classic algorithm, becoming confused when presented with valid but unconventional arrangements of the same operations.

\subsubsection{Example 3: Dot Product with Error Handling (Sample 4723)}

The dot product implementation exhibits the most dramatic vulnerability in our case studies. Despite achieving a maximum score of 4 in its original form, adversarial reordering reduced performance to a minimum score of 2—a 50\% degradation. The existence of 100 unique reorderings (our tracking maximum) indicates extensive independence among operations, particularly in the element-wise multiplication phase before summation. This severe performance drop on a conceptually simple operation reveals how models may rely heavily on memorized patterns rather than understanding the mathematical commutativity of the underlying operations. The error handling components add additional reorderable segments, creating a rich transformation space that thoroughly confuses the evaluated models.

\subsection{Summary on the first experiment}

Our evaluation demonstrates that Large Language Models are susceptible to semantic-preserving adversarial code reordering. All tested models showed a degradation in performance on a code summarization task when faced with these transformations, with Llama 3.2 3B and Gemma 3 12B experiencing the most significant drops in their evaluation scores. \\
The effectiveness of the adversarial attacks appears to correlate with the complexity of the code and the number of possible unique perturbations, as seen in the case studies. For instance, a simple stack implementation with limited reordering possibilities showed no change in performance, while more complex examples like Dijkstra's algorithm and a dot product function with error handling, which allowed for a high number of unique reorderings, exhibited a substantial decrease in their evaluation scores. This indicates that the structural arrangement of code, even when functionally identical, influences the models' comprehension.

\section{Evaluation Using Unit Tests (SAFIM Dataset)}

To complement our LLM-as-a-judge evaluation, we conducted additional experiments using the SAFIM dataset, which provides executable unit tests for objective evaluation. This approach eliminates potential biases from judge models and provides deterministic pass/fail results based on functional correctness.

\subsection{Experimental Configuration}

We evaluated nine models on the SAFIM block completion dataset: BigCode StarCoderBase 7B, DeepseekCoder 1.3B and 6.7B, GPT 3.5 - turbo, InCoder 1B, Phi 2, Wizard Coder Python 13B, Magicoder S CL 7B,
Mixtral 8x7B Instruct.
Key differences from our LLM-as-a-judge experiments:
\begin{itemize}
    \item \textbf{Evaluation metric}: Binary pass/fail based on unit test execution
    \item \textbf{Adversarial attempts}: 20 attempts per sample in comparison to 20 + 4*20. Since there is no scale like but only a Passed/Failed metric, we cannot neighborhood sampling strategies
    \item \textbf{Sampling strategy}: We conducted experiments with both temperature=0.2 (following SAFIM defaults) and deterministic sampling (do\_sample=False) to isolate the effects of our perturbations
\end{itemize}

\subsection{Overall Results}

Table \ref{tab:safim-overall-performance} presents the vulnerability rates across all tested models with the initial SAFIM setting of temperature=0.2.

\begin{table}[h]
\centering
\caption{SAFIM evaluation results across models}
\label{tab:safim-overall-performance}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Original} & \textbf{Eligible for} & \textbf{Vulnerable} & \textbf{Robust} & \textbf{Vulnerability} \\
 & \textbf{Pass Rate} & \textbf{Attack} & \textbf{Samples} & \textbf{Samples} & \textbf{Rate} \\
\midrule
StarCoderBase 7B & 45.91\% & 357/805 & 19 & 338 & 5.32\% \\
DeepseekCoder 1.3B & 29.32\% & 233/805 & 26 & 207 & 11.16\% \\
DeepseekCoder 6.7B & 29.57\% & 235/805 & 26 & 209 & 11.06\% \\
InCoder 1B & 23.09\% & 183/805 & 17 & 166 & 9.29\% \\
GPT 3.5 - turbo & 15.53\% & 123/805 & 0 & 123 & 0\% \\
Phi 2 & 30.06\% & 235/805 & 16 & 219 & 6.81\% \\
Wizard Coder Python 13B & 16.27\% & 129/805 & 0 & 129 & 0\% \\
Magicoder S CL 7B & 16.27\% & 129/805 & 0 & 129 & 0\% \\
Mixtral 8x7B Instruct v0.1 & 16.89\% & 124/746 & 0 & 124 & 0\% \\
\bottomrule
\end{tabular}
\end{table}



\subsection{Sampling Strategy Analysis}

To ensure our results reflect the impact of code perturbations rather than sampling randomness, we conducted experiments with deterministic sampling (do\_sample=False):

\begin{table}[h]
\centering
\caption{Impact of sampling strategy on vulnerability rates}
\label{tab:sampling-comparison}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Temperature=0.2} & \textbf{Deterministic} \\
 & \textbf{Vulnerability Rate} & \textbf{Vulnerability Rate} \\
\midrule
StarCoderBase 7B & 5.07\% & 5.72\% \\
DeepseekCoder 1.3B & 11.16\% & 11.16\% \\
DeepseekCoder 6.7B & 11.06\% & 11.49\% \\
InCoder 1B & 9.29\% & 9.29\% \\
GPT 3.5 - turbo & 0\% & 0\% \\
Phi 2 & 6.88\% & 6.81\% \\
Wizard Coder Python 13B & 0\% & 0\% \\
Magicoder S CL 7B & 0\% & 0\% \\
Mixtral 8x7B Instruct v0.1 & 0\% & 0\% \\
\bottomrule
\end{tabular}
\end{table}

The deterministic results do not vary significantly from the ones that use sampling. This confirms that the vulnerability rates stem from the perturbations and not the sampling in the respective model.

\subsection{Comparison with LLM-as-a-Judge Results}

The SAFIM experiments reveal several important contrasts with our LLM-as-a-judge evaluation:

The SAFIM experiments reveal several important contrasts with our LLM-as-a-judge evaluation. Most notably, models demonstrate substantially lower vulnerability rates when evaluated through unit tests (5-10\%) compared to LLM judge assessments (27-52\%), suggesting different aspects of robustness are being measured. This discrepancy extends to the nature of failures themselves: while unit test failures definitively indicate functional incorrectness, decreases in judge scores may reflect violations of stylistic conventions or structural preferences rather than semantic errors. Furthermore, the baseline performance metrics differ significantly between approaches, with original pass rates on SAFIM ranging from 15-50\%—considerably lower than the completion quality scores of 2.7-3.2 out of 5 observed in judge-based evaluation. This gap underscores the stricter nature of executable validation compared to subjective quality assessment.


\subsection{Characterization of Vulnerable Samples}

Our detailed analysis of the vulnerable samples (19 with StarCoderBase 7B for example) reveals some patterns in how perturbations affect functional correctness. When our adversarial transformations successfully degraded performance, they triggered substantial test failures (averaging 1.47 failures out of 1.74 tests per sample with StarCoderBase 7B). Perhaps more troubling, many vulnerable samples experienced complete functional breakdown, failing all associated unit tests after perturbation. This pattern suggests that when models do lose robustness to our transformations, the failure tends to be catastrophic rather than gradual, indicating a fundamental misunderstanding of the code structure rather than minor errors.


\subsection{Key Findings from Unit Test Evaluation}

The unit test evaluation reveals fundamental insights about model robustness that challenge our initial judge-based findings. Models demonstrate remarkably higher functional robustness, with vulnerability rates of only 5-10\% compared to the 27-52\% observed in judge-based evaluation. This dramatic difference underscores how profoundly the choice of evaluation methodology impacts our understanding of model capabilities. When our attacks do succeed against unit tests, they represent genuine functional failures—the generated code simply does not work as intended, rather than merely violating stylistic conventions. The substantially lower vulnerability rate in this objective evaluation suggests that most semantic-preserving transformations leave the model's core ability to generate functionally correct code intact.


\subsection{Summary on the second experiment}


The SAFIM experiments provide a complementary perspective on model robustness using objective, execution-based evaluation. While vulnerability rates are lower than in judge-based evaluation, the failures represent more severe breakdowns in code understanding. These results suggest a layered model of code comprehension where models rely on syntactic patterns for stylistic decisions while maintaining a more robust functional understanding. This interpretation is reinforced by the observation that judge-based metrics appear to conflate stylistic preferences with genuine semantic understanding, penalizing valid code that deviates from expected patterns. Nevertheless, the persistence of a 5-10\% vulnerability rate even under strict functional evaluation indicates that a small but significant portion of model behavior remains fragile to semantic-preserving perturbations, representing cases where surface-level pattern matching interferes with core code comprehension.

\section{Summary of Key Findings}

Our experiments using both LLM-as-a-judge and unit test evaluation reveal several critical insights about the robustness of code LLMs to semantic-preserving transformations:

\subsection{Observed Vulnerability to Syntactic Variations}
Some models evaluated in our experiments demonstrated measurable vulnerability to semantic-preserving perturbations, though the severity varied considerably across architectures and evaluation methods. Judge-based evaluation revealed performance degradations ranging from 0.24 to 0.62 points on a 5-point scale, with 27\% to 52.7\% of code samples experiencing quality deterioration under adversarial reordering. Crucially, these vulnerabilities persisted even under deterministic sampling conditions, confirming that the observed fragility stems from fundamental model characteristics rather than sampling artifacts.

\subsection{Model Size Does Not Guarantee Robustness}
Our results challenge the conventional assumption that larger models inherently possess greater robustness. Surprisingly, Gemma 3 12B, the largest model in our evaluation set, exhibited the highest vulnerability rate at 52.7\%, while the smaller Gemma 3 4B demonstrated superior resilience with only 27\% vulnerability despite having significantly fewer parameters. This counterintuitive finding suggests that current training methodologies fail to confer robustness as a natural consequence of scale, and may even introduce additional fragilities as models grow larger.


\subsection{Evaluation Methodology Dramatically Affects Results}
The divergence between our two evaluation approaches shows fundamental challenges in assessing code understanding. LLM-as-judge evaluation yielded vulnerability rates between 27-52\%, while unit test evaluation showed dramatically lower rates of 0-10.5\%—a difference of 5-10x that cannot be attributed to experimental variance. This gap suggests that models successfully preserve functional correctness while losing stylistic or structural coherence under perturbation. Judge models appear to heavily penalize syntactic variations that have no impact on program behavior, revealing their own biases toward specific code patterns rather than true semantic evaluation.

\subsection{Characteristics of Vulnerable Code}
Our analysis revealed clear patterns in code structures most susceptible to adversarial reordering. Complex algorithms featuring numerous independent statements, exemplified by our Dijkstra's algorithm implementation with 89 possible valid reorderings, consistently showed higher vulnerability to perturbation. In contrast, simple, tightly coupled code such as basic stack operations with only 4 possible reorderings maintained robust performance across all tested models. With samples averaging 68 unique valid reorderings, our dataset reveals a substantial transformation space that models must navigate, suggesting that real-world code offers ample opportunity for semantic-preserving variations that can confuse current LLMs.


\subsection{Implications for Code Understanding}

The results of our evaluation present a nuanced picture of Large Language Model code comprehension. It appears that some of these models rely on common syntactic patterns they encountered during their training. This is highlighted by the findings in both experiments, which indicate an incomplete abstraction of the code's deeper meaning; true semantic understanding would likely require an invariance to dependency-preserving transformations, a quality not all of the evaluated models did consistently achieve. These findings carry significant implications for the deployment of LLMs in production coding environments. In such settings, robustness to syntactic variations is crucial for reliable performance across diverse codebases and evolving coding styles.

\section{Theoretical Implications for Code Understanding in LLMs}

Our findings contribute to a growing body of evidence suggesting that current LLMs operate through a complex interplay of pattern matching and emergent understanding, with the balance heavily favoring the former in code comprehension tasks. The vulnerability to semantic-preserving transformations indicates that these models have not developed true abstract representations of program semantics, instead relying on statistical regularities in their training data.

This observation connects to fundamental questions about what it means for a system to "understand" code. Human programmers possess an abstract mental model of program execution that remains invariant under semantic-preserving transformations. They understand that statement order matters only when dependencies exist, and they can mentally trace these dependencies to verify correctness. Our results suggest that some of the current LLMs lack this abstract reasoning capability, instead operating through sophisticated but ultimately brittle pattern matching.

The discrepancy between functional and stylistic robustness hints at an interesting parallel with human cognition. Humans also show some sensitivity to code style and formatting, often finding certain arrangements more readable or maintainable than others. However, humans can explicitly recognize and reason about this distinction, understanding when stylistic preferences are separate from semantic requirements. LLMs appear to conflate these levels, treating stylistic patterns as semantic requirements.

These theoretical insights point toward fundamental limitations in the transformer architecture for code understanding. While attention mechanisms can capture long-range dependencies in principle, our results suggest they may not be learning the right kinds of dependencies in practice. The models appear to learn correlational patterns ("this statement usually comes before that one") rather than causal relationships ("this statement must come before that one because of data flow").


\chapter{Conclusion and Outlook}

\section{Conclusion}

This thesis investigated the robustness of Large Language Models for code understanding and generation tasks when confronted with semantically equivalent but syntactically different code variants. Through systematic application of graph-based program analysis and topological sorting, we developed a framework for generating semantic-preserving code transformations that reveal some limitations in how current LLMs process and understand code.

\subsection{Summary of Contributions}

Our work makes several key contributions to the understanding of LLM robustness in code domains:

\textbf{1. Enhanced Program Graph Framework}
We extended existing program graph representations with crucial dependency tracking for imports, control blocks, and jump statements. These enhancements enable accurate identification of all semantic dependencies in Python code, expanding the space of valid transformations from often just the original ordering to a larger perturbation space.

\textbf{2. Adversarial Code Generation Pipeline}
We developed a complete pipeline for generating semantic-preserving adversarial examples through non-deterministic topological sorting for exploring the transformation space, neighborhood sampling for targeted adversarial search, and rigorous validation of semantic preservation. This pipeline enables systematic exploration of the vast space of valid code transformations while maintaining program correctness.


\textbf{3. Comprehensive Robustness Evaluation}
Through experiments on multiple state-of-the-art models using both subjective (LLM-as-judge) and objective (unit tests) evaluation metrics, we demonstrated that some of the tested models exhibit vulnerability to semantic-preserving transformations. Our findings reveal that vulnerability rates vary dramatically based on evaluation methodology, ranging from 5-10\% for functional correctness to 27-52\% for code quality assessment. Furthermore, model size shows no correlation with robustness to syntactic variations, challenging assumptions about the benefits of scale.

\subsection{Answering the Research Questions}

\textbf{RQ1: To what extent are current state-of-the-art LLMs robust to semantic-preserving transformations in code?}

Our results suggest that the evaluated LLMs show limitations in robust understanding of code semantics under our experimental conditions. Even simple transformations like statement reordering and commutative operation swapping can degrade model performance. This vulnerability comes up across different model architectures and sizes, suggesting a fundamental limitation in how these models learn code representations.

\textbf{RQ2: Can we systematically generate adversarial examples that reveal weaknesses in LLM code understanding?}

Yes, our graph-based approach successfully generates adversarial examples that expose model weaknesses while guaranteeing semantic preservation. The combination of enhanced dependency tracking and topological sorting provides a principled method for exploring the space of valid code transformations. Our neighborhood sampling algorithm further refines this search, finding particularly effective adversarial examples.

\textbf{RQ3: Do models that perform better overall on benchmark tasks also demonstrate greater robustness?}

Surprisingly, our experiments did not reveal a clear correlation between benchmark performance and robustness in the tested models. Gemma 3 12B, despite being the largest model with presumably better benchmark scores, showed the highest vulnerability rate (52.7\%). This suggests that current training and evaluation practices do not promote true semantic understanding of code.

\subsection{Implications}

Our findings have significant implications for the deployment and development of code LLMs:

\textbf{For Practitioners:}
Our findings carry important implications for practitioners deploying LLMs in production environments. The demonstrated sensitivity to syntactic variations in code context suggests that careful prompt engineering with attention to statement ordering may significantly improve model outputs. Additionally, evaluating model responses across multiple syntactically different but semantically equivalent prompt variations could enhance reliability and reveal potential failure modes before deployment.

\textbf{For Researchers:}
The research community should reconsider how current benchmarks evaluate code understanding, as they may not adequately test semantic robustness. Our results suggest that training on syntactically diverse but semantically equivalent code samples could improve model robustness. Furthermore, the fundamental limitations revealed by our experiments indicate that new architectures explicitly designed to model program semantics, rather than surface patterns, may be necessary for achieving true code understanding.

\section{Outlook and Future Work}

\subsection{Immediate Extensions}

Several natural extensions to this work could provide additional insights:

\textbf{1. Broader Language Coverage}
While our implementation focuses on Python, extending to other programming languages would reveal whether vulnerability patterns are language-specific or universal. Languages with different syntactic flexibility (e.g., C++ vs. Ruby) may show different robustness profiles.

\textbf{2. Additional Transformation Types}
Beyond statement reordering and commutative operations, the space of semantic-preserving transformations remains largely unexplored. Future work could investigate loop transformations such as converting between for and while loops, substituting equivalent API calls that achieve the same functionality, and systematically inserting or removing dead code to test model sensitivity to irrelevant information. Each transformation type would reveal different aspects of how models understand and process code structure.


\textbf{3. Defensive Training Strategies}
Investigating whether exposure to adversarially reordered code during training improves model robustness represents a promising research direction. This could involve augmenting training data with semantic-preserving transformations, developing adversarial training objectives that explicitly reward invariance to valid reorderings, and implementing contrastive learning approaches that teach models to recognize equivalent code variants as semantically identical.


\subsection{Long-term Research Directions}

\textbf{1. Semantic-Aware Architectures}
Our findings highlight the need for model architectures that explicitly incorporate program semantics rather than relying solely on surface patterns. The enhanced program graphs developed by \textcite{Geisler2023} for handling directed graphs provide a promising foundation for this direction. Their work on Transformers for directed graphs, which preserves directional information through Magnetic Laplacian eigenvectors, could be extended to create architectures that inherently understand code dependencies and control flow. 

Building on this foundation, future architectures could integrate program analysis directly into the model structure. For instance, graph neural networks operating on our enhanced program dependency graphs could be combined with transformers to create hybrid architectures that leverage both local dependency information and global context. The key insight from \textcite{Geisler2023} —that directionality is crucial for understanding program semantics—suggests that future models should move beyond treating code as sequences or undirected graphs.


\textbf{2. Robust Evaluation Frameworks}
Future benchmarks should specifically test semantic understanding through systematic variations, incorporating datasets with multiple valid implementations of identical functionality and developing evaluation metrics that explicitly reward semantic invariance over syntactic similarity.

\textbf{3. Theoretical Understanding}
Developing formal frameworks to explain model failures on semantic-preserving transformations requires deeper investigation into what models actually learn from code, how they construct internal representations of program structure, and how these limitations connect to similar challenges in logical reasoning tasks that require semantic rather than syntactic understanding.

\subsection{Broader Impact}

As LLMs become increasingly integrated into software development workflows, ensuring their robustness to semantic-preserving transformations becomes critical across multiple dimensions. Code security concerns arise from potential adversarial manipulations that could introduce subtle vulnerabilities while maintaining surface-level correctness. Development reliability requires consistent model performance across diverse codebases and coding styles, which our results suggest current models cannot guarantee. Finally, building truly helpful AI-assisted programming tools demands models that understand code semantics deeply enough to remain invariant to superficial syntactic variations.


\subsection{Final Remarks}

This thesis provides empirical evidence suggesting that despite impressive benchmark performance, the tested models show varying levels of robustness to semantic-preserving transformations, indicating potential limitations in semantic understanding. Our graph-based framework for generating semantic-preserving perturbations provides both a diagnostic tool for assessing model robustness and a foundation for developing more robust approaches.

The gap between functional correctness (5-10\% vulnerability) and stylistic consistency (27-52\% vulnerability) suggests that models have learned some aspects of program semantics but remain brittle to syntactic variations. Closing this gap represents a fundamental challenge for the field, requiring advances in model architectures, training procedures, and evaluation methodologies.

As the software industry increasingly adopts AI-powered coding assistants, addressing these robustness limitations becomes not just an academic exercise but a practical necessity for building reliable, trustworthy systems that truly understand the code they generate and manipulate.


\appendix

\backmatter{}

\listoffigures
\listofalgorithms
\addcontentsline{toc}{chapter}{List of Algorithms}



\printbibliography{} % print bibliography

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-engine: default
%%% TeX-command-extra-options: "-shell-escape"
%%% ispell-local-dictionary: "american"
%%% eval: (setenv "TEXINPUTS" ".//:")
%%% TeX-master: t
%%% End:





